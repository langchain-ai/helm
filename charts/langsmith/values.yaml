# Default values for the langsmith helm chart. Refer to documentation on individual values for help with configuration.

# -- Provide a name in place of `langsmith`
nameOverride: ""
# -- String to fully override `"langsmith.fullname"`
fullnameOverride: ""
# -- Annotations that will be applied to all resources created by the chart
commonAnnotations: {}
# -- Labels that will be applied to all resources created by the chart
commonLabels: {}

images:
   # -- Secrets with credentials to pull images from a private registry. Specified as name: value.
  imagePullSecrets: []
  backendImage:
    repository: "docker.io/langchain/langsmith-backend"
    pullPolicy: IfNotPresent
    tag: "0.6.9"
  frontendImage:
    repository: "docker.io/langchain/langsmith-frontend"
    pullPolicy: IfNotPresent
    tag: "0.6.9"
  platformBackendImage:
    repository: "docker.io/langchain/langsmith-go-backend"
    pullPolicy: IfNotPresent
    tag: "0.6.9"
  playgroundImage:
    repository: "docker.io/langchain/langsmith-playground"
    pullPolicy: IfNotPresent
    tag: "0.6.9"
  postgresImage:
    repository: "docker.io/postgres"
    pullPolicy: IfNotPresent
    tag: "14.7"
  redisImage:
    repository: "docker.io/redis"
    pullPolicy: IfNotPresent
    tag: "7"
  clickhouseImage:
    repository: "docker.io/clickhouse/clickhouse-server"
    pullPolicy: Always
    tag: "24.2"

ingress:
  enabled: false
  hostname: ""
  subdomain: ""
  ingressClassName: ""
  annotations: {}
  labels: {}
  tls: []

apiIngress:
  enabled: false
  hostname: ""
  subdomain: ""
  ingressClassName: ""
  annotations: {}
  labels: {}
  tls: []

config:
  existingSecretName: ""
  langsmithLicenseKey: ""
  # -- Salt used to generate the API key. Should be a random string.
  apiKeySalt: ""
  logLevel: "info"
  # -- OpenAI API key. Optional. Only used to power natural language search feature.
  openaiApiKey: ""
  orgCreationDisabled: false
  oauth:
    enabled: false
    oauthClientId: ""
    oauthIssuerUrl: ""
  ttl:
    enabled: false
    upgrade_enabled: false
    ttl_period_seconds:
      # -- 400 day longlived and 14 day shortlived
      longlived: "34560000"
      shortlived: "1209600"

backend:
  name: "backend"
  containerPort: 1984
  existingConfigMapName: ""
  deployment:
    replicas: 1
    labels: {}
    annotations: {}
    podSecurityContext: {}
    securityContext: {}
    resources: {}
#      limits:
#        cpu: 1000m
#        memory: 1Gi
#      requests:
#        cpu: 200m
#        memory: 500Mi
    command:
      - "uvicorn"
      - "app.main:app"
      - "--host"
      - "0.0.0.0"
      - "--port"
      - "$(PORT)"
      - "--log-level"
      - "$(LOG_LEVEL)"
      - "--loop"
      - "uvloop"
      - "--http"
      - "httptools"
      - "--no-access-log"
    startupProbe:
      httpGet:
        path: /ok
        port: 1984
      failureThreshold: 6
      periodSeconds: 10
      timeoutSeconds: 1
    livenessProbe:
      httpGet:
        path: /ok
        port: 1984
      failureThreshold: 6
      periodSeconds: 10
      timeoutSeconds: 1
    readinessProbe:
      httpGet:
        path: /ok
        port: 1984
      failureThreshold: 6
      periodSeconds: 10
      timeoutSeconds: 1
    extraContainerConfig: {}
    extraEnv: []
    sidecars: []
    nodeSelector: {}
    tolerations: []
    affinity: {}
    volumes: []
    volumeMounts: []
  migrations:
    enabled: true
    labels: {}
    annotations: {}
    podSecurityContext: {}
    securityContext: {}
    resources: {}
#      limits:
#        cpu: 1000m
#        memory: 1Gi
#      requests:
#        cpu: 200m
#        memory: 500Mi
    command:
      - "/bin/bash"
      - "-c"
      - "alembic upgrade head"
    extraContainerConfig: {}
    extraEnv: []
    sidecars: []
    nodeSelector: {}
    tolerations: []
    affinity: {}
    volumes: []
    volumeMounts: []
  clickhouseMigrations:
    enabled: true
    labels: {}
    annotations: {}
    podSecurityContext: {}
    securityContext: {}
    resources: {}
#      limits:
#        cpu: 1000m
#        memory: 1Gi
#      requests:
#        cpu: 200m
#        memory: 500Mi
    command:
      - "/bin/bash"
      - "-c"
      - "sleep 20s; migrate -source file://clickhouse/migrations -database 'clickhouse://$(CLICKHOUSE_HOST):$(CLICKHOUSE_NATIVE_PORT)?username=$(CLICKHOUSE_USER)&password=$(CLICKHOUSE_PASSWORD)&database=$(CLICKHOUSE_DB)&x-multi-statement=true&x-migrations-table-engine=MergeTree' up"
    extraContainerConfig: {}
    extraEnv: []
    sidecars: []
    nodeSelector: {}
    tolerations: []
    affinity: {}
    volumes: []
    volumeMounts: []
  autoscaling:
    enabled: false
    # If enabled, use the following values to configure the HPA. You can also use your own HPA configuration by not creating an HPA.
    # You may want to manage the HPA yourself if you have a custom autoscaling setup like KEDA.
    createHpa: true
    minReplicas: 1
    maxReplicas: 5
    targetCPUUtilizationPercentage: 80
  service:
    type: ClusterIP
    port: 1984
    labels: {}
    annotations: {}
    loadBalancerSourceRanges: []
    loadBalancerIP: ""
  serviceAccount:
    create: true
    name: ""
    labels: {}
    annotations: {}

clickhouse:
  name: "clickhouse"
  external:
    # If enabled, use the following values to connect to an external database. This will also disable the
    # creation of a clickhouse stateful-set and service.
    enabled: false
    host: ""
    port: "8123"
    nativePort: "9000"
    user: "default"
    password: "password"
    database: "default"
    tls: false
    existingSecretName: ""
  containerHttpPort: 8123
  containerNativePort: 9000
  statefulSet:
    labels: {}
    annotations: {}
    podSecurityContext: {}
    securityContext: {}
    resources: {}
    #      limits:
    #        cpu: 5000m
    #        memory: 20Gi
    #      requests:
    #        cpu: 3000m
    #        memory: 12Gi
    command:
      - "/bin/bash"
      - "-c"
      - "sed 's/id -g/id -gn/' /entrypoint.sh > /tmp/entrypoint.sh; exec bash /tmp/entrypoint.sh"
    startupProbe:
      httpGet:
        path: /ping
        port: 8123
      failureThreshold: 6
      periodSeconds: 10
      timeoutSeconds: 1
    readinessProbe:
      httpGet:
        path: /ping
        port: 8123
      failureThreshold: 6
      periodSeconds: 10
      timeoutSeconds: 1
    livenessProbe:
      httpGet:
        path: /ping
        port: 8123
      failureThreshold: 6
      periodSeconds: 10
      timeoutSeconds: 1
    extraContainerConfig: {}
    extraEnv: []
    sidecars: []
    nodeSelector: {}
    tolerations: []
    affinity: {}
    volumes: []
    volumeMounts: []
    # We recommend using a persistent volume and increasing the storage size to something like 50Gi when using in a production environment!
    persistence:
      size: 50Gi
      storageClassName: ""
  service:
    type: ClusterIP
    httpPort: 8123
    nativePort: 9000
    labels: {}
    annotations: {}
    loadBalancerSourceRanges: []
    loadBalancerIP: ""
  serviceAccount:
    create: true
    name: ""
    labels: {}
    annotations: {}

frontend:
  name: "frontend"
  containerPort: 8080
  existingConfigMapName: ""
  deployment:
    replicas: 1
    labels: {}
    annotations: {}
    podSecurityContext: {}
    securityContext: {}
    resources: {}
#      limits:
#        cpu: 1000m
#        memory: 1Gi
#      requests:
#        cpu: 200m
#        memory: 500Mi
    command:
      - "/entrypoint.sh"
    startupProbe:
      httpGet:
        path: /health
        port: 8080
      failureThreshold: 6
      periodSeconds: 10
      timeoutSeconds: 1
    livenessProbe:
      httpGet:
        path: /health
        port: 8080
      failureThreshold: 6
      periodSeconds: 10
      timeoutSeconds: 1
    readinessProbe:
      httpGet:
        path: /health
        port: 8080
      failureThreshold: 6
      periodSeconds: 10
      timeoutSeconds: 1
    extraContainerConfig: {}
    extraEnv: []
    sidecars: []
    nodeSelector: {}
    tolerations: []
    affinity: {}
    volumes: []
    volumeMounts: []
  autoscaling:
    enabled: false
    # If enabled, use the following values to configure the HPA. You can also use your own HPA configuration by not creating an HPA.
    # You may want to manage the HPA yourself if you have a custom autoscaling setup like KEDA.
    createHpa: true
    minReplicas: 1
    maxReplicas: 5
    targetCPUUtilizationPercentage: 80
  service:
    type: LoadBalancer
    httpPort: 80
    httpsPort: 443
    labels: {}
    annotations: {}
    loadBalancerSourceRanges: []
    loadBalancerIP: ""
  serviceAccount:
    create: true
    name: ""
    labels: {}
    annotations: {}

playground:
  name: "playground"
  containerPort: 3001
  deployment:
    replicas: 1
    labels: {}
    annotations: {}
    podSecurityContext: {}
    securityContext: {}
    resources: {}
#      limits:
#        cpu: 1000m
#        memory: 1Gi
#      requests:
#        cpu: 200m
#        memory: 500Mi
    command:
      - "yarn"
      - "start"
    startupProbe:
      httpGet:
        path: /ok
        port: 3001
      failureThreshold: 6
      periodSeconds: 10
      timeoutSeconds: 1
    livenessProbe:
      httpGet:
        path: /ok
        port: 3001
      failureThreshold: 6
      periodSeconds: 10
      timeoutSeconds: 1
    readinessProbe:
      httpGet:
        path: /ok
        port: 3001
      failureThreshold: 6
      periodSeconds: 10
      timeoutSeconds: 1
    extraContainerConfig: {}
    extraEnv: []
    sidecars: []
    nodeSelector: {}
    tolerations: []
    affinity: {}
    volumes: []
    volumeMounts: []
  autoscaling:
    enabled: false
    # If enabled, use the following values to configure the HPA. You can also use your own HPA configuration by not creating an HPA.
    # You may want to manage the HPA yourself if you have a custom autoscaling setup like KEDA.
    createHpa: true
    minReplicas: 1
    maxReplicas: 5
    targetCPUUtilizationPercentage: 80
  service:
    type: ClusterIP
    port: 3001
    labels: {}
    annotations: {}
    loadBalancerSourceRanges: []
    loadBalancerIP: ""
  serviceAccount:
    create: true
    name: ""
    labels: {}
    annotations: {}

postgres:
  name: "postgres"
  external:
    # If enabled, use the following values to connect to an external database. This will also disable the
    # creation of a postgres stateful-set and service.
    enabled: false
    host: ""
    port: "5432"
    user: "postgres"
    password: "postgres"
    database: "postgres"
    schema: "public"
    # If connection string is specified, we will ignore all above values and use the connection string instead.
    # Do not include the driver name(something like "postgres://" in the connection string.
    connectionUrl: ""
    existingSecretName: ""
  containerPort: 5432
  statefulSet:
    labels: {}
    annotations: {}
    podSecurityContext: {}
    securityContext: {}
    resources: {}
#      limits:
#        cpu: 4000m
#        memory: 16Gi
#      requests:
#        cpu: 1000m
#        memory: 4Gi
    command: []
    startupProbe:
      exec:
        command:
          - /bin/sh
          - -c
          - exec pg_isready -d postgres -U postgres
      failureThreshold: 6
      periodSeconds: 10
      timeoutSeconds: 1
    readinessProbe:
      exec:
        command:
          - /bin/sh
          - -c
          - exec pg_isready -d postgres -U postgres
      failureThreshold: 6
      periodSeconds: 10
      timeoutSeconds: 1
    livenessProbe:
      exec:
        command:
          - /bin/sh
          - -c
          - exec pg_isready -d postgres -U postgres
      failureThreshold: 6
      periodSeconds: 10
      timeoutSeconds: 1
    extraContainerConfig: {}
    extraEnv: []
    sidecars: []
    nodeSelector: {}
    tolerations: []
    affinity: {}
    volumes: []
    volumeMounts: []
    persistence:
      size: 8Gi
      storageClassName: ""
  service:
    type: ClusterIP
    port: 5432
    labels: {}
    annotations: {}
    loadBalancerSourceRanges: []
    loadBalancerIP: ""
  serviceAccount:
    create: true
    name: ""
    labels: {}
    annotations: {}

queue:
  name: "queue"
  deployment:
    replicas: 3
    labels: {}
    annotations: {}
    podSecurityContext: {}
    securityContext: {}
    resources: {}
#      limits:
#        cpu: 2000m
#        memory: 4Gi
#      requests:
#        cpu: 1000m
#        memory: 2000Mi
    command:
      - "saq"
      - "app.workers.queues.single_queue_worker.settings"
      - "--quiet"
    extraContainerConfig: {}
    extraEnv: []
    sidecars: []
    nodeSelector: {}
    tolerations: []
    affinity: {}
    volumes: []
    volumeMounts: []
  autoscaling:
    enabled: false
    # If enabled, use the following values to configure the HPA. You can also use your own HPA configuration by not creating an HPA.
    # You may want to manage the HPA yourself if you have a custom autoscaling setup like KEDA.
    createHpa: true
    minReplicas: 3
    maxReplicas: 10
    targetCPUUtilizationPercentage: 80
  serviceAccount:
    create: true
    name: ""
    labels: {}
    annotations: {}

platformBackend:
  name: "platform-backend"
  containerPort: 1986
  existingConfigMapName: ""
  deployment:
    replicas: 1
    labels: {}
    annotations: {}
    podSecurityContext: {}
    securityContext: {}
    resources: {}
#      limits:
#        cpu: 1000m
#        memory: 1Gi
#      requests:
#        cpu: 200m
#        memory: 500Mi
    command:
      - "./smith-go"
    startupProbe:
      httpGet:
        path: /ok
        port: 1986
      failureThreshold: 6
      periodSeconds: 10
      timeoutSeconds: 1
    livenessProbe:
      httpGet:
        path: /ok
        port: 1986
      failureThreshold: 6
      periodSeconds: 10
      timeoutSeconds: 1
    readinessProbe:
      httpGet:
        path: /ok
        port: 1986
      failureThreshold: 6
      periodSeconds: 10
      timeoutSeconds: 1
    extraContainerConfig: {}
    extraEnv: []
    sidecars: []
    nodeSelector: {}
    tolerations: []
    affinity: {}
    volumes: []
    volumeMounts: []
  autoscaling:
    enabled: false
    # If enabled, use the following values to configure the HPA. You can also use your own HPA configuration by not creating an HPA.
    # You may want to manage the HPA yourself if you have a custom autoscaling setup like KEDA.
    createHpa: true
    minReplicas: 1
    maxReplicas: 5
    targetCPUUtilizationPercentage: 80
  service:
    type: ClusterIP
    port: 1986
    labels: {}
    annotations: {}
    loadBalancerSourceRanges: []
    loadBalancerIP: ""
  serviceAccount:
    create: true
    name: ""
    labels: {}
    annotations: {}

redis:
  name: "redis"
  external:
    # If enabled, use the following values to connect to an external redis instance. This will also disable the
    # creation of a redis stateful-set and service.
    enabled: false
    connectionUrl: ""
    existingSecretName: ""
  containerPort: 6379
  statefulSet:
    labels: {}
    annotations: {}
    podSecurityContext: {}
    securityContext: {}
    resources: {}
#      limits:
#        cpu: 4000m
#        memory: 16Gi
#      requests:
#        cpu: 1000m
#        memory: 2Gi
    command: []
    startupProbe:
      exec:
        command:
          - /bin/sh
          - -c
          - exec redis-cli ping
      failureThreshold: 6
      periodSeconds: 10
      timeoutSeconds: 1
    readinessProbe:
      exec:
        command:
          - /bin/sh
          - -c
          - exec redis-cli ping
      failureThreshold: 6
      periodSeconds: 10
      timeoutSeconds: 1
    livenessProbe:
      exec:
        command:
          - /bin/sh
          - -c
          - exec redis-cli ping
      failureThreshold: 6
      periodSeconds: 10
      timeoutSeconds: 1
    extraContainerConfig: {}
    extraEnv: []
    sidecars: []
    nodeSelector: {}
    tolerations: []
    affinity: {}
    volumes: []
    volumeMounts: []
    persistence:
      enabled: false
      size: 8Gi
      storageClassName: ""
  service:
    type: ClusterIP
    port: 6379
    labels: {}
    annotations: {}
    loadBalancerSourceRanges: []
    loadBalancerIP: ""
  serviceAccount:
    create: true
    name: ""
    labels: {}
    annotations: {}
