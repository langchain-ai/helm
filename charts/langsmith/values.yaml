# Default values for the langsmith helm chart. Refer to documentation on individual values for help with configuration.

# -- Provide a name in place of `langsmith`
nameOverride: ""
# -- String to fully override `"langsmith.fullname"`
fullnameOverride: ""
# -- Annotations that will be applied to all resources created by the chart
commonAnnotations: {}
# -- Labels that will be applied to all resources created by the chart
commonLabels: {}
# -- Common environment variables that will be applied to all deployments/statefulsets except for the playground/aceBackend services (which are sandboxed). Be careful not to override values already specified by the chart.
commonEnv: []

images:
   # -- Secrets with credentials to pull images from a private registry. Specified as name: value.
  imagePullSecrets: []
  aceBackendImage:
    repository: "docker.io/langchain/langsmith-ace-backend"
    pullPolicy: IfNotPresent
    tag: "0.10.20"
  backendImage:
    repository: "docker.io/langchain/langsmith-backend"
    pullPolicy: IfNotPresent
    tag: "0.10.20"
  frontendImage:
    repository: "docker.io/langchain/langsmith-frontend"
    pullPolicy: IfNotPresent
    tag: "0.10.20"
  hostBackendImage:
    repository: "docker.io/langchain/hosted-langserve-backend"
    pullPolicy: IfNotPresent
    tag: "0.10.20"
  operatorImage:
    repository: "docker.io/langchain/langgraph-operator"
    pullPolicy: IfNotPresent
    tag: "3b0048a"
  platformBackendImage:
    repository: "docker.io/langchain/langsmith-go-backend"
    pullPolicy: IfNotPresent
    tag: "0.10.20"
  playgroundImage:
    repository: "docker.io/langchain/langsmith-playground"
    pullPolicy: IfNotPresent
    tag: "0.10.20"
  postgresImage:
    repository: "docker.io/postgres"
    pullPolicy: IfNotPresent
    tag: "14.7"
  redisImage:
    repository: "docker.io/redis"
    pullPolicy: IfNotPresent
    tag: "7"
  clickhouseImage:
    repository: "docker.io/clickhouse/clickhouse-server"
    pullPolicy: Always
    tag: "24.8"
  quickwitImage:
    repository: quickwit/quickwit
    pullPolicy: IfNotPresent
    tag: edge

ingress:
  # -- Set to true if langgraph platform enabled.
  enabled: false
  hostname: ""
  subdomain: ""
  ingressClassName: ""
  annotations: {}
  labels: {}
  tls: []

config:
  existingSecretName: ""
  langsmithLicenseKey: ""
  langgraphPlatform:
      # -- Optional. Used to enable the Langgraph platform control plane. If enabled, the license key must be provided.
    enabled: false
    langgraphPlatformLicenseKey: ""
    #  Defaults to ingress.hostname
    rootDomain: ""
    tlsEnabled: true

  # -- Salt used to generate the API key. Should be a random string.
  apiKeySalt: ""
  logLevel: "info"
  # -- Must be 'oauth' for OAuth with PKCE, 'mixed' for basic auth or OAuth without PKCE
  authType: ""
  basicAuth:
    enabled: false
    initialOrgAdminEmail: ""
    # Either set the password and JWT secret in plaintext, or set in the secret specified by existingSecretName above.
    initialOrgAdminPassword: ""
    jwtSecret: ""
  # -- Prevent organization creation. If using basic auth, this is set to true by default.
  orgCreationDisabled: false
  # -- Disable personal orgs. Users will need to be invited to an org manually. If using basic auth, this is set to true by default.
  personalOrgsDisabled: false
  # -- Enable Workspace Admins to invite users to the org and workspace.
  workspaceScopeOrgInvitesEnabled: false

  # -- Base URL of the LangSmith installation. Used for redirects.
  hostname: ""

  oauth:
    enabled: false
    oauthClientId: ""
    # -- Client secret requires authType to be 'mixed' and hostname to be present
    oauthClientSecret: ""
    oauthIssuerUrl: ""
    oauthScopes: "email,profile,openid"  # Comma-separated list of scopes. We require at least 'email', 'profile', and 'openid'. Some OIDC providers require the 'offline_access' scope for refresh tokens.
    oauthSessionMaxSec: "86400"  # 24 hours. Maximum age of a session in seconds. Your session will attempt to refresh until the max age at which point you will be logged out.
  # -- TTL configuration
  # Optional. Used to set TTLS for longlived and shortlived objects.
  ttl:
    enabled: true
    ttl_period_seconds:
      # -- 400 day longlived and 14 day shortlived
      longlived: "34560000"
      shortlived: "1209600"
    #
  # -- Blob storage configuration
  # Optional. Used to store inputs, outputs, and errors in Blob Storage.
  # We currently support S3, GCS, Minio, and Azure as Blob Storage providers.
  blobStorage:
    enabled: false
    engine: "S3"
    # If you are using langsmith-managed-clickhouse, you may not want inputs to be stored in clickhouse for search.
    # Set this as false to ensure that inputs/outputs/errors are not stored in clickhouse.
    # 'clickhouse.external.hybrid: true' overrides this to false.
    chSearchEnabled: true
    # Set this to change the threshold for payloads to be stored in blob storage
    # 'clickhouse.external.hybrid: true' overrides this to 0
    minBlobStorageSizeKb: "20"
    # If you are using workload identity, you may not need to store the S3 credentials in the secrets.
    # Instead, you will need to add the workload identity annotation to the backend and queue service accounts.
    accessKey: ""
    accessKeySecret: ""
    bucketName: ""
    apiURL: "https://s3.us-west-2.amazonaws.com"
    # The following blob storage configuration values are for Azure and require blobStorage.engine = "Azure"
    # -- Optional. Set this along with azureStorageAccountKey to use a storage account and access key. Higher precedence than azureStorageConnectionString.
    azureStorageAccountName: ""
    azureStorageAccountKey: ""
    # -- Required if using Azure blob storage
    azureStorageContainerName: ""
    # -- Optional. Use this to specify the full connection string including any authentication params.
    azureStorageConnectionString: ""
    # -- Optional. Use this to customize the service URL, which by default is 'https://<storage_account_name>.blob.core.windows.net/'
    azureStorageServiceUrlOverride: ""

  # -- Application Settings. These are used to tune the application
  settings:
    # -- Optional. Be very careful when lowering this value as it can result in runs being lost if your queue is down/not processing items fast enough.
    redisRunsExpirySeconds: "43200"  # 12 hours

  fullTextSearch:
    indexing:
      # Enables Experimental Search support
      enabled: false
    deletes:
      # Enables deletes for Experimental Search
      enabled: false

aceBackend:
  name: "ace-backend"
  containerPort: 1987
  bindAddress: "0.0.0.0"
  deployment:
    autoRestart: true
    replicas: 1
    labels: {}
    annotations: {}
    podSecurityContext: {}
    securityContext: {}
    resources:
      limits:
        cpu: 1000m
        memory: 2Gi
      requests:
        cpu: 200m
        memory: 1000Mi
    command:
      - "deno"
      - "run"
      - "--unstable-worker-options"
      - "--allow-env"
      - "--allow-net=$(BIND_ADDRESS):$(PORT)"
      - "--node-modules-dir"
      - "-R"
      - "src/main.ts"
      - "-R"
      - "src/python_worker.ts"
    startupProbe:
      httpGet:
        path: /ok
        port: 1987
      failureThreshold: 6
      periodSeconds: 10
      timeoutSeconds: 1
    livenessProbe:
      httpGet:
        path: /ok
        port: 1987
      failureThreshold: 6
      periodSeconds: 10
      timeoutSeconds: 1
    readinessProbe:
      httpGet:
        path: /ok
        port: 1987
      failureThreshold: 6
      periodSeconds: 10
      timeoutSeconds: 1
    extraContainerConfig: {}
    extraEnv: []
    sidecars: []
    nodeSelector: {}
    tolerations: []
    topologySpreadConstraints: []
    affinity: {}
    volumes: []
    volumeMounts: []
    terminationGracePeriodSeconds: 30
  autoscaling:
    enabled: false
    # If enabled, use the following values to configure the HPA. You can also use your own HPA configuration by not creating an HPA.
    # You may want to manage the HPA yourself if you have a custom autoscaling setup like KEDA.
    createHpa: true
    minReplicas: 1
    maxReplicas: 5
    targetCPUUtilizationPercentage: 50
    targetMemoryUtilizationPercentage: 80
  pdb:
    enabled: false
    minAvailable: 1
    labels: {}
    annotations: {}
  service:
    type: ClusterIP
    port: 1987
    labels: {}
    annotations: {}
    loadBalancerSourceRanges: []
    loadBalancerIP: ""
  serviceAccount:
    create: true
    name: ""
    labels: {}
    annotations: {}


backend:
  name: "backend"
  containerPort: 1984
  existingConfigMapName: ""
  deployment:
    autoRestart: true
    replicas: 1
    labels: {}
    annotations: {}
    podSecurityContext: {}
    securityContext: {}
    resources:
      limits:
        cpu: 2000m
        memory: 4Gi
      requests:
        cpu: 1000m
        memory: 2Gi
    command:
      - "uvicorn"
      - "app.main:app"
      - "--host"
      - "0.0.0.0"
      - "--port"
      - "$(PORT)"
      - "--log-level"
      - "$(LOG_LEVEL)"
      - "--loop"
      - "uvloop"
      - "--http"
      - "httptools"
      - "--no-access-log"
    startupProbe:
      httpGet:
        path: /health
        port: 1984
      failureThreshold: 6
      periodSeconds: 10
      timeoutSeconds: 1
    livenessProbe:
      httpGet:
        path: /health
        port: 1984
      failureThreshold: 6
      periodSeconds: 10
      timeoutSeconds: 1
    readinessProbe:
      httpGet:
        path: /health
        port: 1984
      failureThreshold: 6
      periodSeconds: 10
      timeoutSeconds: 1
    extraContainerConfig: {}
    extraEnv: []
    sidecars: []
    nodeSelector: {}
    tolerations: []
    topologySpreadConstraints: []
    affinity: {}
    volumes: []
    volumeMounts: []
    terminationGracePeriodSeconds: 30
  migrations:
    enabled: true
    # Helpful when running using helm template to avoid the job name conflict
    randomizeName: true
    labels: {}
    annotations: {}
    podSecurityContext: {}
    securityContext: {}
    resources:
      limits:
        cpu: 1000m
        memory: 1Gi
      requests:
        cpu: 200m
        memory: 500Mi
    command:
      - "/bin/bash"
      - "-c"
      - "alembic upgrade head"
    extraContainerConfig: {}
    extraEnv: []
    sidecars: []
    nodeSelector: {}
    tolerations: []
    topologySpreadConstraints: []
    affinity: {}
    volumes: []
    volumeMounts: []
    # You can set this to null to disable the TTL for the migrations job. Useful for some deployment setups.
    ttlSecondsAfterFinished: 600
  authBootstrap:
    # Helpful when running using helm template to avoid the job name conflict
    randomizeName: true
    labels: {}
    annotations: {}
    podSecurityContext: {}
    securityContext: {}
    resources:
      limits:
        cpu: 1000m
        memory: 1Gi
      requests:
        cpu: 200m
        memory: 500Mi
    command:
      - "python"
      - "hooks/auth_bootstrap.pyc"
    extraContainerConfig: {}
    extraEnv: []
    sidecars: []
    nodeSelector: {}
    tolerations: []
    topologySpreadConstraints: []
    affinity: {}
    volumes: []
    volumeMounts: []
    # You can set this to null to disable the TTL for the migrations job. Useful for some deployment setups.
    ttlSecondsAfterFinished: 600
  clickhouseMigrations:
    enabled: true
    # Helpful when running using helm template to avoid the job name conflict
    randomizeName: true
    labels: {}
    annotations: {}
    podSecurityContext: {}
    securityContext: {}
    resources:
      limits:
        cpu: 1000m
        memory: 1Gi
      requests:
        cpu: 200m
        memory: 500Mi
    command:
      - "/bin/bash"
      - "scripts/wait_for_clickhouse_and_migrate.sh"
    extraContainerConfig: {}
    extraEnv: []
    sidecars: []
    nodeSelector: {}
    tolerations: []
    topologySpreadConstraints: []
    affinity: {}
    volumes: []
    volumeMounts: []
    # You can set this to null to disable the TTL for the migrations job. Useful for some deployment setups.
    ttlSecondsAfterFinished: 600
  autoscaling:
    enabled: false
    # If enabled, use the following values to configure the HPA. You can also use your own HPA configuration by not creating an HPA.
    # You may want to manage the HPA yourself if you have a custom autoscaling setup like KEDA.
    createHpa: true
    minReplicas: 1
    maxReplicas: 5
    targetCPUUtilizationPercentage: 50
    targetMemoryUtilizationPercentage: 80
  pdb:
    enabled: false
    minAvailable: 1
    labels: {}
    annotations: {}
  service:
    type: ClusterIP
    port: 1984
    labels: {}
    annotations: {}
    loadBalancerSourceRanges: []
    loadBalancerIP: ""
  serviceAccount:
    create: true
    name: ""
    labels: {}
    annotations: {}

clickhouse:
  name: "clickhouse"
  config:
    allowSimdjson: true
  external:
    # If enabled, use the following values to connect to an external database. This will also disable the
    # creation of a clickhouse stateful-set and service.
    enabled: false
    # -- Must be set to true if using managed ClickHouse
    hybrid: false
    host: ""
    hostSecretKey: "clickhouse_host"
    port: "8123"
    portSecretKey: "clickhouse_port"
    nativePort: "9000"
    nativePortSecretKey: "clickhouse_native_port"
    user: "default"
    userSecretKey: "clickhouse_user"
    password: "password"
    passwordSecretKey: "clickhouse_password"
    database: "default"
    databaseSecretKey: "clickhouse_db"
    tls: false
    tlsSecretKey: "clickhouse_tls"
    cluster: ""
    existingSecretName: ""
  containerHttpPort: 8123
  containerNativePort: 9000
  statefulSet:
    labels: {}
    annotations: {}
    podSecurityContext: {}
    securityContext: {}
    resources:
# For a production environment, we recommend increasing the CPU and memory limits to something like 16 cores and 64Gi.
#      limits:
#        cpu: 16000m
#        memory: 64Gi
#      requests:
#        cpu: 8000m
#        memory: 32Gi
      limits:
        cpu: 8000m
        memory: 32Gi
      requests:
        cpu: 3500m
        memory: 12Gi
    command:
      - "/bin/bash"
      - "-c"
      - "sed 's/id -g/id -gn/' /entrypoint.sh > /tmp/entrypoint.sh; exec bash /tmp/entrypoint.sh"
    startupProbe:
      httpGet:
        path: /ping
        port: 8123
      failureThreshold: 6
      periodSeconds: 10
      timeoutSeconds: 1
    readinessProbe:
      httpGet:
        path: /ping
        port: 8123
      failureThreshold: 6
      periodSeconds: 10
      timeoutSeconds: 1
    livenessProbe:
      httpGet:
        path: /ping
        port: 8123
      failureThreshold: 6
      periodSeconds: 10
      timeoutSeconds: 1
    extraContainerConfig: {}
    extraEnv: []
    sidecars: []
    nodeSelector: {}
    tolerations: []
    topologySpreadConstraints: []
    affinity: {}
    volumes: []
    volumeMounts: []
    # We recommend using a persistent volume and increasing the storage size to something like 50Gi when using in a production environment!
    persistence:
      enabled: true
      size: 50Gi
      storageClassName: ""
  pdb:
    enabled: false
    minAvailable: 1
    labels: {}
    annotations: {}
  service:
    type: ClusterIP
    httpPort: 8123
    nativePort: 9000
    labels: {}
    annotations: {}
    loadBalancerSourceRanges: []
    loadBalancerIP: ""
  serviceAccount:
    create: true
    name: ""
    labels: {}
    annotations: {}

frontend:
  name: "frontend"
  containerPort: 8080
  existingConfigMapName: ""
  # Nginx Max Body Size. Refer to https://nginx.org/en/docs/http/ngx_http_core_module.html#client_max_body_size for more information.
  maxBodySize: "25M"
  proxyReadTimeout: "300"
  proxyWriteTimeout: "300"
  proxyConnectTimeout: "60"
  ipv6Enabled: true
  deployment:
    autoRestart: true
    replicas: 1
    labels: {}
    annotations: {}
    podSecurityContext: {}
    securityContext: {}
    resources:
      limits:
        cpu: 1000m
        memory: 2Gi
      requests:
        cpu: 500m
        memory: 1Gi
    command:
      - "/entrypoint.sh"
    startupProbe:
      httpGet:
        path: /health
        port: 8080
      failureThreshold: 10
      periodSeconds: 10
      timeoutSeconds: 1
    livenessProbe:
      httpGet:
        path: /health
        port: 8080
      failureThreshold: 10
      periodSeconds: 10
      timeoutSeconds: 1
    readinessProbe:
      httpGet:
        path: /health
        port: 8080
      failureThreshold: 10
      periodSeconds: 10
      timeoutSeconds: 1
    extraContainerConfig: {}
    extraEnv: []
    sidecars: []
    nodeSelector: {}
    tolerations: []
    topologySpreadConstraints: []
    affinity: {}
    volumes: []
    volumeMounts: []
    terminationGracePeriodSeconds: 30
  autoscaling:
    enabled: false
    # If enabled, use the following values to configure the HPA. You can also use your own HPA configuration by not creating an HPA.
    # You may want to manage the HPA yourself if you have a custom autoscaling setup like KEDA.
    createHpa: true
    minReplicas: 1
    maxReplicas: 5
    targetCPUUtilizationPercentage: 50
    targetMemoryUtilizationPercentage: 80
  pdb:
    enabled: false
    minAvailable: 1
    labels: {}
    annotations: {}
  service:
    type: LoadBalancer
    httpPort: 80
    httpsPort: 443
    labels: {}
    annotations: {}
    loadBalancerSourceRanges: []
    loadBalancerIP: ""
  serviceAccount:
    create: true
    name: ""
    labels: {}
    annotations: {}

hostBackend:
  name: "host-backend"
  containerPort: 1985
  deployment:
    autoRestart: true
    replicas: 1
    labels: {}
    annotations: {}
    podSecurityContext: {}
    securityContext: {}
    resources:
      limits:
        cpu: 1000m
        memory: 2Gi
      requests:
        cpu: 200m
        memory: 1000Mi
    command:
      - "uvicorn"
      - "host.main:app"
      - "--host"
      - "0.0.0.0"
      - "--port"
      - "$(PORT)"
      - "--log-level"
      - "$(LOG_LEVEL)"
      - "--loop"
      - "uvloop"
      - "--http"
      - "httptools"
      - "--no-access-log"
    startupProbe:
      httpGet:
        path: /ok
        port: 1985
      failureThreshold: 6
      periodSeconds: 10
      timeoutSeconds: 1
    livenessProbe:
      httpGet:
        path: /ok
        port: 1985
      failureThreshold: 6
      periodSeconds: 10
      timeoutSeconds: 1
    readinessProbe:
      httpGet:
        path: /ok
        port: 1985
      failureThreshold: 6
      periodSeconds: 10
      timeoutSeconds: 1
    extraContainerConfig: {}
    extraEnv: []
    sidecars: []
    nodeSelector: {}
    tolerations: []
    topologySpreadConstraints: []
    affinity: {}
    volumes: []
    volumeMounts: []
    terminationGracePeriodSeconds: 30
  autoscaling:
    enabled: false
    # If enabled, use the following values to configure the HPA. You can also use your own HPA configuration by not creating an HPA.
    # You may want to manage the HPA yourself if you have a custom autoscaling setup like KEDA.
    createHpa: true
    minReplicas: 1
    maxReplicas: 5
    targetCPUUtilizationPercentage: 50
    targetMemoryUtilizationPercentage: 80
  pdb:
    enabled: false
    minAvailable: 1
    labels: {}
    annotations: {}
  service:
    type: ClusterIP
    port: 1985
    labels: {}
    annotations: {}
    loadBalancerSourceRanges: []
    loadBalancerIP: ""
  serviceAccount:
    create: true
    name: ""
    labels: {}
    annotations: {}
  rbac:
    create: true
    labels: {}
    annotations: {}

listener:
  name: "listener"
  deployment:
    autoRestart: true
    replicas: 1
    labels: {}
    annotations: {}
    podSecurityContext: {}
    securityContext: {}
    resources:
      limits:
        cpu: 2000m
        memory: 4Gi
      requests:
        cpu: 1000m
        memory: 2Gi
    command:
      - "saq"
      - "app.workers.queues.host_worker.settings"
      - "--quiet"
    startupProbe:
      exec:
        command:
          - "saq"
          - "app.workers.queues.host_worker.settings"
          - "--check"
      failureThreshold: 6
      periodSeconds: 60
      timeoutSeconds: 30
    readinessProbe:
      exec:
        command:
          - "saq"
          - "app.workers.queues.host_worker.settings"
          - "--check"
      failureThreshold: 6
      periodSeconds: 60
      timeoutSeconds: 30
    livenessProbe:
      exec:
        command:
          - "saq"
          - "app.workers.queues.host_worker.settings"
          - "--check"
      failureThreshold: 6
      periodSeconds: 60
      timeoutSeconds: 30
    extraContainerConfig: {}
    extraEnv: []
    sidecars: []
    nodeSelector: {}
    tolerations: []
    topologySpreadConstraints: []
    affinity: {}
    volumes: []
    volumeMounts: []
    terminationGracePeriodSeconds: 30
  autoscaling:
    enabled: false
    # If enabled, use the following values to configure the HPA. You can also use your own HPA configuration by not creating an HPA.
    # You may want to manage the HPA yourself if you have a custom autoscaling setup like KEDA.
    createHpa: true
    minReplicas: 3
    maxReplicas: 10
    targetCPUUtilizationPercentage: 50
    targetMemoryUtilizationPercentage: 80
  pdb:
    enabled: false
    minAvailable: 1
    labels: {}
    annotations: {}
  serviceAccount:
    create: true
    name: ""
    labels: {}
    annotations: {}
  rbac:
    create: true
    labels: {}
    annotations: {}
  templates:
    db: |
      apiVersion: apps/v1
      kind: StatefulSet
      metadata:
        name: ${service_name}
      spec:
        serviceName: ${service_name}
        replicas: ${replicas}
        selector:
          matchLabels:
            app: ${service_name}
        persistentVolumeClaimRetentionPolicy:
          whenDeleted: Delete
          whenScaled: Retain
        template:
          metadata:
            labels:
              app: ${service_name}
          spec:
            containers:
            - name: postgres
              image: pgvector/pgvector:pg15
              ports:
              - containerPort: 5432
              command: ["docker-entrypoint.sh"]
              args:
                - postgres
                - -c
                - max_connections=${max_connections}
              env:
              - name: POSTGRES_PASSWORD
                valueFrom:
                  secretKeyRef:
                    name: ${secret_name}
                    key: POSTGRES_PASSWORD
              - name: POSTGRES_USER
                value: ${postgres_user}
              - name: POSTGRES_DB
                value: ${postgres_db}
              - name: PGDATA
                value: /var/lib/postgresql/data/pgdata
              volumeMounts:
              - name: postgres-data
                mountPath: /var/lib/postgresql/data
              resources:
                requests:
                  cpu: "${cpu}"
                  memory: "${memory_mb}Mi"
                limits:
                  cpu: "${cpu_limit}"
                  memory: "${memory_limit}Mi"
            enableServiceLinks: false
        volumeClaimTemplates:
        - metadata:
            name: postgres-data
          spec:
            accessModes: ["ReadWriteOnce"]
            resources:
              requests:
                storage: "${storage_gi}Gi"
    redis: |
      apiVersion: apps/v1
      kind: Deployment
      metadata:
        name: ${service_name}
      spec:
        replicas: 1
        selector:
          matchLabels:
            app: ${service_name}
        template:
          metadata:
            labels:
              app: ${service_name}
          spec:
            containers:
            - name: redis
              image: redis:6
              ports:
              - containerPort: 6379
              livenessProbe:
                exec:
                  command:
                  - redis-cli
                  - ping
                initialDelaySeconds: 30
                periodSeconds: 10
              readinessProbe:
                tcpSocket:
                  port: 6379
                initialDelaySeconds: 10
                periodSeconds: 5
              resources:
                requests:
                  cpu: "1"
                  memory: "2048Mi"
                limits:
                  cpu: "1"
                  memory: "2048Mi"
            enableServiceLinks: false

platformBackend:
  name: "platform-backend"
  containerPort: 1986
  existingConfigMapName: ""
  deployment:
    autoRestart: true
    replicas: 3
    labels: {}
    annotations: {}
    podSecurityContext: {}
    securityContext: {}
    resources:
      limits:
        cpu: 2000m
        memory: 4Gi
      requests:
        cpu: 1000m
        memory: 2Gi
    command:
      - "./smith-go"
    startupProbe:
      httpGet:
        path: /ok
        port: 1986
      failureThreshold: 6
      periodSeconds: 10
      timeoutSeconds: 1
    livenessProbe:
      httpGet:
        path: /ok
        port: 1986
      failureThreshold: 6
      periodSeconds: 10
      timeoutSeconds: 1
    readinessProbe:
      httpGet:
        path: /ok
        port: 1986
      failureThreshold: 6
      periodSeconds: 10
      timeoutSeconds: 1
    extraContainerConfig: {}
    extraEnv: []
    sidecars: []
    nodeSelector: {}
    tolerations: []
    topologySpreadConstraints: []
    affinity: {}
    volumes: []
    volumeMounts: []
    terminationGracePeriodSeconds: 30
  autoscaling:
    enabled: false
    # If enabled, use the following values to configure the HPA. You can also use your own HPA configuration by not creating an HPA.
    # You may want to manage the HPA yourself if you have a custom autoscaling setup like KEDA.
    createHpa: true
    minReplicas: 3
    maxReplicas: 10
    targetCPUUtilizationPercentage: 50
    targetMemoryUtilizationPercentage: 80
  pdb:
    enabled: false
    minAvailable: 1
    labels: {}
    annotations: {}
  service:
    type: ClusterIP
    port: 1986
    labels: {}
    annotations: {}
    loadBalancerSourceRanges: []
    loadBalancerIP: ""
  serviceAccount:
    create: true
    name: ""
    labels: {}
    annotations: {}

operator:
  name: "operator"
  enabled: true
  createCRDs: true
  watchNamespaces: ""
  kedaEnabled: true
  deployment:
    autoRestart: true
    replicas: 1
    labels: {}
    annotations: {}
    podSecurityContext: {}
    securityContext: {}
    resources:
      limits:
        cpu: 2000m
        memory: 4Gi
      requests:
        cpu: 1000m
        memory: 2Gi
    extraContainerConfig: {}
    extraEnv: []
    sidecars: []
    nodeSelector: {}
    tolerations: []
    topologySpreadConstraints: []
    affinity: {}
    volumes: []
    volumeMounts: []
    terminationGracePeriodSeconds: 30
  pdb:
    enabled: false
    minAvailable: 1
  serviceAccount:
    create: true
    name: ""
    labels: {}
    annotations: {}
  rbac:
    create: true
    labels: {}
    annotations: {}
  # These templates are used by the operator as the base spec for creating resources.
  templates:
    deployment: |
      apiVersion: apps/v1
      kind: Deployment
      metadata:
        name: ${name}
        namespace: ${namespace}
      spec:
        replicas: ${replicas}
        selector:
          matchLabels:
            app: ${name}
        template:
          metadata:
            labels:
              app: ${name}
          spec:
            enableServiceLinks: false
            containers:
            - name: api-server
              image: ${image}
              ports:
              - name: api-server
                containerPort: 8000
                protocol: TCP
              livenessProbe:
                httpGet:
                  path: /lgp/${name}/ok?check_db=1
                  port: 8000
                periodSeconds: 15
                timeoutSeconds: 5
                failureThreshold: 6
              readinessProbe:
                httpGet:
                  path: /lgp/${name}/ok
                  port: 8000
                periodSeconds: 15
                timeoutSeconds: 5
                failureThreshold: 6
    service: |
      apiVersion: v1
      kind: Service
      metadata:
        name: ${name}
        namespace: ${namespace}
      spec:
        type: ClusterIP
        selector:
          app: ${name}
        ports:
        - name: api-server
          protocol: TCP
          port: 8000
          targetPort: 8000

playground:
  name: "playground"
  containerPort: 1988
  deployment:
    autoRestart: true
    replicas: 1
    labels: {}
    annotations: {}
    podSecurityContext: {}
    securityContext: {}
    resources:
      limits:
        cpu: 1000m
        memory: 8Gi
      requests:
        cpu: 500m
        memory: 1Gi
    command:
      - "uvicorn"
      - "playground.main:app"
      - "--host"
      - "0.0.0.0"
      - "--port"
      - "$(PORT)"
      - "--log-level"
      - "$(LOG_LEVEL)"
      - "--loop"
      - "uvloop"
      - "--http"
      - "httptools"
      - "--no-access-log"
    startupProbe:
      httpGet:
        path: /ok
        port: 1988
      failureThreshold: 6
      periodSeconds: 10
      timeoutSeconds: 1
    livenessProbe:
      httpGet:
        path: /ok
        port: 1988
      failureThreshold: 6
      periodSeconds: 10
      timeoutSeconds: 1
    readinessProbe:
      httpGet:
        path: /ok
        port: 1988
      failureThreshold: 6
      periodSeconds: 10
      timeoutSeconds: 1
    extraContainerConfig: {}
    extraEnv: []
    sidecars: []
    nodeSelector: {}
    tolerations: []
    topologySpreadConstraints: []
    affinity: {}
    volumes: []
    volumeMounts: []
    terminationGracePeriodSeconds: 30
  autoscaling:
    enabled: false
    # If enabled, use the following values to configure the HPA. You can also use your own HPA configuration by not creating an HPA.
    # You may want to manage the HPA yourself if you have a custom autoscaling setup like KEDA.
    createHpa: true
    minReplicas: 1
    maxReplicas: 5
    targetCPUUtilizationPercentage: 50
    targetMemoryUtilizationPercentage: 80
  pdb:
    enabled: false
    minAvailable: 1
    labels: {}
    annotations: {}
  service:
    type: ClusterIP
    port: 1988
    labels: {}
    annotations: {}
    loadBalancerSourceRanges: []
    loadBalancerIP: ""
  serviceAccount:
    create: true
    name: ""
    labels: {}
    annotations: {}

postgres:
  name: "postgres"
  external:
    # If enabled, use the following values to connect to an external database. This will also disable the
    # creation of a postgres stateful-set and service.
    enabled: false
    host: ""
    port: "5432"
    user: "postgres"
    password: "postgres"
    database: "postgres"
    schema: "public"
    # If connection string is specified, we will ignore all above values and use the connection string instead.
    # Do not include the driver name(something like "postgres://" in the connection string.
    connectionUrl: ""
    connectionUrlSecretKey: "connection_url"
    existingSecretName: ""
  containerPort: 5432
  statefulSet:
    labels: {}
    annotations: {}
    podSecurityContext: {}
    securityContext: {}
    resources:
      # In a production environment, we strongly recommend an external Postgres database.
      limits:
        cpu: 4000m
        memory: 16Gi
      requests:
        cpu: 2000m
        memory: 8Gi
    command: []
    startupProbe:
      exec:
        command:
          - /bin/sh
          - -c
          - exec pg_isready -d postgres -U postgres
      failureThreshold: 6
      periodSeconds: 10
      timeoutSeconds: 1
    readinessProbe:
      exec:
        command:
          - /bin/sh
          - -c
          - exec pg_isready -d postgres -U postgres
      failureThreshold: 6
      periodSeconds: 10
      timeoutSeconds: 1
    livenessProbe:
      exec:
        command:
          - /bin/sh
          - -c
          - exec pg_isready -d postgres -U postgres
      failureThreshold: 6
      periodSeconds: 10
      timeoutSeconds: 1
    extraContainerConfig: {}
    extraEnv: []
    sidecars: []
    nodeSelector: {}
    tolerations: []
    topologySpreadConstraints: []
    affinity: {}
    volumes: []
    volumeMounts: []
    persistence:
      enabled: true
      size: 8Gi
      storageClassName: ""
  pdb:
    enabled: false
    minAvailable: 1
    labels: {}
    annotations: {}
  service:
    type: ClusterIP
    port: 5432
    labels: {}
    annotations: {}
    loadBalancerSourceRanges: []
    loadBalancerIP: ""
  serviceAccount:
    create: true
    name: ""
    labels: {}
    annotations: {}

queue:
  name: "queue"
  deployment:
    autoRestart: true
    replicas: 3
    labels: {}
    annotations: {}
    podSecurityContext: {}
    securityContext: {}
    resources:
      limits:
        cpu: 2000m
        memory: 4Gi
      requests:
        cpu: 1000m
        memory: 2Gi
    command:
      - "saq"
      - "app.workers.queues.single_queue_worker.settings"
      - "--quiet"
    startupProbe:
      exec:
        command:
          - "saq"
          - "app.workers.queues.single_queue_worker.settings"
          - "--check"
      failureThreshold: 6
      periodSeconds: 60
      timeoutSeconds: 30
    readinessProbe:
      exec:
        command:
          - "saq"
          - "app.workers.queues.single_queue_worker.settings"
          - "--check"
      failureThreshold: 6
      periodSeconds: 60
      timeoutSeconds: 30
    livenessProbe:
      exec:
        command:
          - "saq"
          - "app.workers.queues.single_queue_worker.settings"
          - "--check"
      failureThreshold: 6
      periodSeconds: 60
      timeoutSeconds: 30
    extraContainerConfig: {}
    extraEnv: []
    sidecars: []
    nodeSelector: {}
    tolerations: []
    topologySpreadConstraints: []
    affinity: {}
    volumes: []
    volumeMounts: []
    terminationGracePeriodSeconds: 30
  pdb:
    enabled: false
    minAvailable: 1
    labels: {}
    annotations: {}
  autoscaling:
    enabled: false
    # If enabled, use the following values to configure the HPA. You can also use your own HPA configuration by not creating an HPA.
    # You may want to manage the HPA yourself if you have a custom autoscaling setup like KEDA.
    createHpa: true
    minReplicas: 3
    maxReplicas: 10
    targetCPUUtilizationPercentage: 50
    targetMemoryUtilizationPercentage: 80
  serviceAccount:
    create: true
    name: ""
    labels: {}
    annotations: {}

redis:
  name: "redis"
  external:
    # If enabled, use the following values to connect to an external redis instance. This will also disable the
    # creation of a redis stateful-set and service.
    enabled: false
    connectionUrl: ""
    connectionUrlSecretKey: "connection_url"
    existingSecretName: ""
  containerPort: 6379
  statefulSet:
    labels: {}
    annotations: {}
    podSecurityContext: {}
    securityContext: {}
    # In a production environment, we strongly recommend an external Redis database.
    resources:
      limits:
        cpu: 4000m
        memory: 8Gi
      requests:
        cpu: 2000m
        memory: 4Gi
    command: []
    startupProbe:
      exec:
        command:
          - /bin/sh
          - -c
          - exec redis-cli ping
      failureThreshold: 6
      periodSeconds: 10
      timeoutSeconds: 1
    readinessProbe:
      exec:
        command:
          - /bin/sh
          - -c
          - exec redis-cli ping
      failureThreshold: 6
      periodSeconds: 10
      timeoutSeconds: 1
    livenessProbe:
      exec:
        command:
          - /bin/sh
          - -c
          - exec redis-cli ping
      failureThreshold: 6
      periodSeconds: 10
      timeoutSeconds: 1
    extraContainerConfig: {}
    extraEnv: []
    sidecars: []
    nodeSelector: {}
    tolerations: []
    topologySpreadConstraints: []
    affinity: {}
    volumes: []
    volumeMounts: []
    persistence:
      enabled: true
      size: 8Gi
      storageClassName: ""
  pdb:
    enabled: false
    minAvailable: 1
    labels: {}
    annotations: {}
  service:
    type: ClusterIP
    port: 6379
    labels: {}
    annotations: {}
    loadBalancerSourceRanges: []
    loadBalancerIP: ""
  serviceAccount:
    create: true
    name: ""
    labels: {}
    annotations: {}

quickwit:
  name: quickwit

  # -- Additional labels to add to all resources
  additionalLabels: {}

  serviceAccount:
    # Specifies whether a service account should be created
    create: true
    # Annotations to add to the service account
    annotations: {}
    # The name of the service account to use.
    # If not set and create is true, a name is generated using the fullname template
    name: ""

  annotations: {}

  podAnnotations: {}

  podSecurityContext:
    fsGroup: 1005

  securityContext:
    runAsNonRoot: true
    runAsUser: 1005

  # Additional global env
  environment: {}
    # KEY: VALUE
  environmentFrom: []
    # - secretRef:
    #     name: quickwit
    # - configMapRef:
    #     name: quickwit

  configMaps: []
    # - name: configmap1
    #   mountPath: /quickwit/configmaps/

  # Global affinity settings applied to all deployments
  affinity: {}

  service:
    # Service type configuration default for all Quickwit services
    type: ClusterIP

    # -- Set the ip family policy to configure dual-stack see [Configure dual-stack](https://kubernetes.io/docs/concepts/services-networking/dual-stack/#services)
    ipFamilyPolicy: ""
    # -- Sets the families that should be supported and the order in which they should be applied to ClusterIP as well. Can be IPv4 and/or IPv6.
    ipFamilies: []

    annotations: {}

  ports:
  - name: rest
    containerPort: 7280
    protocol: TCP
  - name: grpc
    containerPort: 7281
    protocol: TCP
  - name: discovery
    containerPort: 7282
    protocol: UDP

  probes:
    startupProbe:
      httpGet:
        path: /health/livez
        port: rest
      failureThreshold: 12
      periodSeconds: 5
    livenessProbe:
      httpGet:
        path: /health/livez
        port: rest
    readinessProbe:
      httpGet:
        path: /health/readyz
        port: rest

  searcher:
    name: quickwit-searcher
    serviceAnnotations: {}
    # serviceType: ClusterIP

    statefulSet:
      replicas: 1

      # Extra env for searcher
      extraEnv: []
      extraEnvFrom: []
        # - secretRef:
        #     name: quickwit-searcher
        # - configMapRef:
        #     name: quickwit-searcher

      # extraVolumes -- Additional volumes to use with Pods.
      extraVolumes: []

      # extraVolumeMounts -- Additional volumes to mount into Quickwit containers.
      extraVolumeMounts: []

      resources:
        limits:
          cpu: 2000m
          memory: 8Gi
        requests:
          cpu: 1000m
          memory: 4Gi

      persistentVolume:
        enabled: false
        # storage: "1Gi"
        # storageClass: ""

      # Configure size limit of the emptyDir (only when using ephemeral volume)
      emptyDir: {}
        # sizeLimit: 20Gi

      updateStrategy: {}
        # type: RollingUpdate

      annotations: {}

      podAnnotations: {}


      nodeSelector: {}

      tolerations: []

      affinity: {}

    # Pod disruption budget (either maxUnavailable or minAvailable can be set, not both)
    pdb:
      enabled: false
      maxUnavailable: 1
      # minAvailable: 2
      labels: {}
      annotations: {}

  indexer:
    name: quickwit-indexer
    serviceAnnotations: {}
    # serviceType: ClusterIP

    statefulSet:
      replicas: 1

      # Extra env for indexer
      extraEnv: []
      extraEnvFrom: []
        # - secretRef:
        #     name: quickwit-indexer
        # - configMapRef:
        #     name: quickwit-indexer

      # extraVolumes -- Additional volumes to use with Pods.
      extraVolumes: []

      # extraVolumeMounts -- Additional volumes to mount into Quickwit containers.
      extraVolumeMounts: []

      resources:
        limits:
          cpu: 2000m
          memory: 8Gi
        requests:
          cpu: 1000m
          memory: 4Gi

      updateStrategy: {}
        # type: RollingUpdate

      annotations: {}

      podAnnotations: {}

      nodeSelector: {}

      tolerations: []

      affinity: {}

      # Long grace period is recommended to wait for all index commit_timeout_secs and splits to be published
      # See https://quickwit.io/docs/configuration/index-config#indexing-settings
      terminationGracePeriodSeconds: 120

      # Configure size limit of the emptyDir (only when using ephemeral volume)
      emptyDir: {}
        # sizeLimit: 20Gi

      persistentVolume:
        enabled: false
        # storage: "1Gi"
        # storageClass: ""

    # Pod disruption budget (either maxUnavailable or minAvailable can be set, not both)
    pdb:
      enabled: false
      maxUnavailable: 1
      # minAvailable: 2
      labels: {}
      annotations: {}

  metastore:
    name: quickwit-metastore
    serviceAnnotations: {}
    # serviceType: ClusterIP

    postgres:
      # If set to true, the metastore will use the same Postgres DB instance as Langsmith services
      shareWithLangSmith: false
      # Existing secret with metastore postgres
      existingSecretName: ""
      connectionUrlSecretKey: "metastore_connection_url"

    deployment:
      replicas: 1

      # Extra env for metastore
      extraEnv: []
      # This is the recommended way to inject `QW_METASTORE_URI` when using the postgres metastore (see https://quickwit.io/docs/configuration/metastore-config)
      extraEnvFrom: []
        # - secretRef:
        #     name: quickwit-metastore
        # - configMapRef:
        #     name: quickwit-metastore

      emptyDir: {}

      # extraVolumes -- Additional volumes to use with Pods.
      extraVolumes: []

      # extraVolumeMounts -- Additional volumes to mount into Quickwit containers.
      extraVolumeMounts: []

      resources:
        limits:
          cpu: 1000m
          memory: 2Gi
        requests:
          cpu: 200m
          memory: 1Gi

      updateStrategy: {}
        # type: RollingUpdate

      annotations: {}

      podAnnotations: {}

      nodeSelector: {}

      tolerations: []

      affinity: {}

  controlPlane:
    name: quickwit-control-plane
    serviceAnnotations: {}
    # serviceType: ClusterIP

    deployment:
      # Extra env for control plane
      extraEnv: []
      extraEnvFrom: []
        # - secretRef:
        #     name: quickwit-control-plane
        # - configMapRef:
        #     name: quickwit-control-plane

      emptyDir: {}

      # extraVolumes -- Additional volumes to use with Pods.
      extraVolumes: []

      # extraVolumeMounts -- Additional volumes to mount into Quickwit containers.
      extraVolumeMounts: []

      resources:
        limits:
          cpu: 1000m
          memory: 2Gi
        requests:
          cpu: 200m
          memory: 1Gi

      annotations: {}

      podAnnotations: {}

      nodeSelector: {}

      tolerations: []

      affinity: {}

  janitor:
    name: quickwit-janitor
    serviceAnnotations: {}
    # serviceType: ClusterIP

    deployment:
      # Extra env for janitor
      extraEnv: []
      extraEnvFrom: []
        # - secretRef:
        #     name: quickwit-janitor
        # - configMapRef:
        #     name: quickwit-janitor

      emptyDir: {}

      # extraVolumes -- Additional volumes to use with Pods.
      extraVolumes: []

      # extraVolumeMounts -- Additional volumes to mount into Quickwit containers.
      extraVolumeMounts: []

      resources:
        limits:
          cpu: 1000m
          memory: 2Gi
        requests:
          cpu: 200m
          memory: 1Gi

      annotations: {}

      podAnnotations: {}

      nodeSelector: {}

      tolerations: []

      affinity: {}

  updateRunsIndex:
    name: quickwit-update-runs-index

  # Quickwit configuration
  # Warning: This config is writed directly into a configMap
  # to avoid passing sensitive value you can pass environment variables.
  # https://quickwit.io/docs/configuration/node-config#using-environment-variables-in-the-configuration
  configLocation: /quickwit/node.yaml

  config:
    version: 0.8
    listen_address: 0.0.0.0
    gossip_listen_port: 7282
    # data_dir: /quickwit/qwdata
    # default_index_root_uri: s3://quickwit/indexes

    # postgres:
    #   max_num_connections: 50

    # storage:
      # s3:
        # endpoint: "http://custom-s3-endpoint"
        # region: eu-east-1
        # We recommend using IAM roles and permissions to access Amazon S3 resources,
        # but you can specify a pair of access and secret keys if necessary.
        # access_key_id: <my access key>
        # secret_access_key: ${AWS_ACCESS_KEY_ID}
      # azure:
        # account: "<my account name>"
        # access_key: ${QW_AZURE_STORAGE_ACCESS_KEY}

    # Indexer settings
    # indexer:
    #   split_store_max_num_bytes: 200G
    #   split_store_max_num_splits: 10000
    # Ingest API settings
    # ingest_api:
    #   max_queue_memory_usage: 2GiB
    #   max_queue_disk_usage: 4GiB
    # Searcher settings
    # searcher:
    #   fast_field_cache_capacity: 10G
    #   split_footer_cache_capacity: 1G
    #   max_num_concurrent_split_streams: 100
