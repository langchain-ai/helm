# Default values for the langsmith helm chart. Refer to documentation on individual values for help with configuration.

# -- Provide a name in place of `langsmith`
nameOverride: ""
# -- String to fully override `"langsmith.fullname"`
fullnameOverride: ""
# -- Annotations that will be applied to all resources created by the chart
commonAnnotations: {}
# -- Labels that will be applied to all resources created by the chart
commonLabels: {}
# -- Common environment variables that will be applied to all deployments/statefulsets except for the playground/aceBackend services (which are sandboxed). Be careful not to override values already specified by the chart.
commonEnv: []

images:
   # -- Secrets with credentials to pull images from a private registry. Specified as name: value.
  imagePullSecrets: []
  aceBackendImage:
    repository: "docker.io/langchain/langsmith-ace-backend"
    pullPolicy: IfNotPresent
    tag: "0.8.92"
  backendImage:
    repository: "docker.io/langchain/langsmith-backend"
    pullPolicy: IfNotPresent
    tag: "0.8.92"
  frontendImage:
    repository: "docker.io/langchain/langsmith-frontend"
    pullPolicy: IfNotPresent
    tag: "0.8.92"
  platformBackendImage:
    repository: "docker.io/langchain/langsmith-go-backend"
    pullPolicy: IfNotPresent
    tag: "0.8.92"
  playgroundImage:
    repository: "docker.io/langchain/langsmith-playground"
    pullPolicy: IfNotPresent
    tag: "0.8.92"
  postgresImage:
    repository: "docker.io/postgres"
    pullPolicy: IfNotPresent
    tag: "14.7"
  redisImage:
    repository: "docker.io/redis"
    pullPolicy: IfNotPresent
    tag: "7"
  clickhouseImage:
    repository: "docker.io/clickhouse/clickhouse-server"
    pullPolicy: Always
    tag: "24.5"

ingress:
  enabled: false
  hostname: ""
  subdomain: ""
  ingressClassName: ""
  annotations: {}
  labels: {}
  tls: []

# Only enable this ingress if you would like to expose the backend as a separate service.
# By default, this is not needed as the frontend ingress will proxy requests to the backend.
apiIngress:
  enabled: false
  hostname: ""
  subdomain: ""
  ingressClassName: ""
  annotations: {}
  labels: {}
  tls: []

config:
  existingSecretName: ""
  langsmithLicenseKey: ""
  # -- Salt used to generate the API key. Should be a random string.
  apiKeySalt: ""
  logLevel: "info"
  # -- Must be 'oauth' for OAuth with PKCE, 'mixed' for basic auth or OAuth without PKCE
  authType: ""
  basicAuth:
    enabled: false
    initialOrgAdminEmail: ""
    # Either set the password and JWT secret in plaintext, or set in the secret specified by existingSecretName above.
    initialOrgAdminPassword: ""
    jwtSecret: ""
  # -- Prevent organization creation. If using basic auth, this is set to true by default.
  orgCreationDisabled: false
  # -- Disable personal orgs. Users will need to be invited to an org manually. If using basic auth, this is set to true by default.
  personalOrgsDisabled: false
  # -- Enable Workspace Admins to invite users to the org and workspace.
  workspaceScopeOrgInvitesEnabled: false

  # -- Base URL of the LangSmith installation. Used for redirects.
  hostname: ""

  oauth:
    enabled: false
    oauthClientId: ""
    # -- Client secret requires authType to be 'mixed' and hostname to be present
    oauthClientSecret: ""
    oauthIssuerUrl: ""
  # -- TTL configuration
  # Optional. Used to set TTLS for longlived and shortlived objects.
  ttl:
    enabled: true
    ttl_period_seconds:
      # -- 400 day longlived and 14 day shortlived
      longlived: "34560000"
      shortlived: "1209600"
    #
  # -- Blob storage configuration
  # Optional. Used to store inputs, outputs, and errors in Blob Storage.
  # We currently support S3, GCS, Minio, and Azure as Blob Storage providers.
  blobStorage:
    enabled: false
    engine: "S3"
    # If you are using langsmith-managed-clickhouse, you may not want inputs to be stored in clickhouse for search.
    # Set this as false to ensure that inputs/outputs/errors are not stored in clickhouse.
    # 'clickhouse.external.hybrid: true' overrides this to false.
    chSearchEnabled: true
    # Set this to change the threshold for payloads to be stored in blob storage
    # 'clickhouse.external.hybrid: true' overrides this to 0
    minBlobStorageSizeKb: "20"
    # If you are using workload identity, you may not need to store the S3 credentials in the secrets.
    # Instead, you will need to add the workload identity annotation to the backend and queue service accounts.
    accessKey: ""
    accessKeySecret: ""
    bucketName: ""
    apiURL: "https://s3.us-west-2.amazonaws.com"
    # The following blob storage configuration values are for Azure and require blobStorage.engine = "Azure"
    # -- Optional. Set this along with azureStorageAccountKey to use a storage account and access key. Higher precedence than azureStorageConnectionString.
    azureStorageAccountName: ""
    azureStorageAccountKey: ""
    # -- Required if using Azure blob storage
    azureStorageContainerName: ""
    # -- Optional. Use this to specify the full connection string including any authentication params.
    azureStorageConnectionString: ""
    # -- Optional. Use this to customize the service URL, which by default is 'https://<storage_account_name>.blob.core.windows.net/'
    azureStorageServiceUrlOverride: ""

  # -- Application Settings. These are used to tune the application
  settings:
    # -- Optional. Be very careful when lowering this value as it can result in runs being lost if your queue is down/not processing items fast enough.
    redisRunsExpirySeconds: "43200"  # 12 hours

aceBackend:
  name: "ace-backend"
  containerPort: 1987
  bindAddress: "0.0.0.0"
  deployment:
    autoRestart: true
    replicas: 1
    labels: {}
    annotations: {}
    podSecurityContext: {}
    securityContext: {}
    resources:
      limits:
        cpu: 1000m
        memory: 2Gi
      requests:
        cpu: 200m
        memory: 1000Mi
    command:
      - "deno"
      - "run"
      - "--unstable-worker-options"
      - "--allow-env"
      - "--allow-net=$(BIND_ADDRESS):$(PORT)"
      - "--node-modules-dir"
      - "-R"
      - "src/main.ts"
      - "-R"
      - "src/python_worker.ts"
    startupProbe:
      httpGet:
        path: /ok
        port: 1987
      failureThreshold: 6
      periodSeconds: 10
      timeoutSeconds: 1
    livenessProbe:
      httpGet:
        path: /ok
        port: 1987
      failureThreshold: 6
      periodSeconds: 10
      timeoutSeconds: 1
    readinessProbe:
      httpGet:
        path: /ok
        port: 1987
      failureThreshold: 6
      periodSeconds: 10
      timeoutSeconds: 1
    extraContainerConfig: {}
    extraEnv: []
    sidecars: []
    nodeSelector: {}
    tolerations: []
    topologySpreadConstraints: []
    affinity: {}
    volumes: []
    volumeMounts: []
    terminationGracePeriodSeconds: 30
  autoscaling:
    enabled: false
    # If enabled, use the following values to configure the HPA. You can also use your own HPA configuration by not creating an HPA.
    # You may want to manage the HPA yourself if you have a custom autoscaling setup like KEDA.
    createHpa: true
    minReplicas: 1
    maxReplicas: 5
    targetCPUUtilizationPercentage: 50
    targetMemoryUtilizationPercentage: 80
  service:
    type: ClusterIP
    port: 1987
    labels: {}
    annotations: {}
    loadBalancerSourceRanges: []
    loadBalancerIP: ""
  serviceAccount:
    create: true
    name: ""
    labels: {}
    annotations: {}


backend:
  name: "backend"
  containerPort: 1984
  existingConfigMapName: ""
  deployment:
    autoRestart: true
    replicas: 1
    labels: {}
    annotations: {}
    podSecurityContext: {}
    securityContext: {}
    resources:
      limits:
        cpu: 2000m
        memory: 4Gi
      requests:
        cpu: 1000m
        memory: 2Gi
    command:
      - "uvicorn"
      - "app.main:app"
      - "--host"
      - "0.0.0.0"
      - "--port"
      - "$(PORT)"
      - "--log-level"
      - "$(LOG_LEVEL)"
      - "--loop"
      - "uvloop"
      - "--http"
      - "httptools"
      - "--no-access-log"
    startupProbe:
      httpGet:
        path: /health
        port: 1984
      failureThreshold: 6
      periodSeconds: 10
      timeoutSeconds: 1
    livenessProbe:
      httpGet:
        path: /health
        port: 1984
      failureThreshold: 6
      periodSeconds: 10
      timeoutSeconds: 1
    readinessProbe:
      httpGet:
        path: /health
        port: 1984
      failureThreshold: 6
      periodSeconds: 10
      timeoutSeconds: 1
    extraContainerConfig: {}
    extraEnv: []
    sidecars: []
    nodeSelector: {}
    tolerations: []
    topologySpreadConstraints: []
    affinity: {}
    volumes: []
    volumeMounts: []
    terminationGracePeriodSeconds: 30
  migrations:
    enabled: true
    # Helpful when running using helm template to avoid the job name conflict
    randomizeName: true
    labels: {}
    annotations: {}
    podSecurityContext: {}
    securityContext: {}
    resources: {}
#      limits:
#        cpu: 1000m
#        memory: 1Gi
#      requests:
#        cpu: 200m
#        memory: 500Mi
    command:
      - "/bin/bash"
      - "-c"
      - "alembic upgrade head"
    extraContainerConfig: {}
    extraEnv: []
    sidecars: []
    nodeSelector: {}
    tolerations: []
    topologySpreadConstraints: []
    affinity: {}
    volumes: []
    volumeMounts: []
    # You can set this to null to disable the TTL for the migrations job. Useful for some deployment setups.
    ttlSecondsAfterFinished: 600
  authBootstrap:
    # Helpful when running using helm template to avoid the job name conflict
    randomizeName: true
    labels: {}
    annotations: {}
    podSecurityContext: {}
    securityContext: {}
    resources: {}
    #      limits:
    #        cpu: 1000m
    #        memory: 1Gi
    #      requests:
    #        cpu: 200m
    #        memory: 500Mi
    command:
      - "python"
      - "hooks/auth_bootstrap.pyc"
    extraContainerConfig: {}
    extraEnv: []
    sidecars: []
    nodeSelector: {}
    tolerations: []
    topologySpreadConstraints: []
    affinity: {}
    volumes: []
    volumeMounts: []
    # You can set this to null to disable the TTL for the migrations job. Useful for some deployment setups.
    ttlSecondsAfterFinished: 600
  clickhouseMigrations:
    enabled: true
    # Helpful when running using helm template to avoid the job name conflict
    randomizeName: true
    labels: {}
    annotations: {}
    podSecurityContext: {}
    securityContext: {}
    resources: {}
#      limits:
#        cpu: 1000m
#        memory: 1Gi
#      requests:
#        cpu: 200m
#        memory: 500Mi
    command:
      - "/bin/bash"
      - "scripts/wait_for_clickhouse_and_migrate.sh"
    extraContainerConfig: {}
    extraEnv: []
    sidecars: []
    nodeSelector: {}
    tolerations: []
    topologySpreadConstraints: []
    affinity: {}
    volumes: []
    volumeMounts: []
    # You can set this to null to disable the TTL for the migrations job. Useful for some deployment setups.
    ttlSecondsAfterFinished: 600
  autoscaling:
    enabled: false
    # If enabled, use the following values to configure the HPA. You can also use your own HPA configuration by not creating an HPA.
    # You may want to manage the HPA yourself if you have a custom autoscaling setup like KEDA.
    createHpa: true
    minReplicas: 1
    maxReplicas: 5
    targetCPUUtilizationPercentage: 50
    targetMemoryUtilizationPercentage: 80
  service:
    type: ClusterIP
    port: 1984
    labels: {}
    annotations: {}
    loadBalancerSourceRanges: []
    loadBalancerIP: ""
  serviceAccount:
    create: true
    name: ""
    labels: {}
    annotations: {}

clickhouse:
  name: "clickhouse"
  config:
    allowSimdjson: true
  external:
    # If enabled, use the following values to connect to an external database. This will also disable the
    # creation of a clickhouse stateful-set and service.
    enabled: false
    # -- Must be set to true if using managed ClickHouse
    hybrid: false
    host: ""
    port: "8123"
    nativePort: "9000"
    user: "default"
    password: "password"
    database: "default"
    tls: false
    cluster: ""
    existingSecretName: ""
  containerHttpPort: 8123
  containerNativePort: 9000
  statefulSet:
    labels: {}
    annotations: {}
    podSecurityContext: {}
    securityContext: {}
    resources:
# For a production environment, we recommend increasing the CPU and memory limits to something like 16 cores and 64Gi.
#      limits:
#        cpu: 16000m
#        memory: 64Gi
#      requests:
#        cpu: 8000m
#        memory: 32Gi
      limits:
        cpu: 8000m
        memory: 32Gi
      requests:
        cpu: 3500m
        memory: 12Gi
    command:
      - "/bin/bash"
      - "-c"
      - "sed 's/id -g/id -gn/' /entrypoint.sh > /tmp/entrypoint.sh; exec bash /tmp/entrypoint.sh"
    startupProbe:
      httpGet:
        path: /ping
        port: 8123
      failureThreshold: 6
      periodSeconds: 10
      timeoutSeconds: 1
    readinessProbe:
      httpGet:
        path: /ping
        port: 8123
      failureThreshold: 6
      periodSeconds: 10
      timeoutSeconds: 1
    livenessProbe:
      httpGet:
        path: /ping
        port: 8123
      failureThreshold: 6
      periodSeconds: 10
      timeoutSeconds: 1
    extraContainerConfig: {}
    extraEnv: []
    sidecars: []
    nodeSelector: {}
    tolerations: []
    topologySpreadConstraints: []
    affinity: {}
    volumes: []
    volumeMounts: []
    # We recommend using a persistent volume and increasing the storage size to something like 50Gi when using in a production environment!
    persistence:
      size: 50Gi
      storageClassName: ""
  service:
    type: ClusterIP
    httpPort: 8123
    nativePort: 9000
    labels: {}
    annotations: {}
    loadBalancerSourceRanges: []
    loadBalancerIP: ""
  serviceAccount:
    create: true
    name: ""
    labels: {}
    annotations: {}

frontend:
  name: "frontend"
  containerPort: 8080
  existingConfigMapName: ""
  # Nginx Max Body Size. Refer to https://nginx.org/en/docs/http/ngx_http_core_module.html#client_max_body_size for more information.
  maxBodySize: "25M"
  proxyReadTimeout: "300"
  proxyWriteTimeout: "300"
  proxyConnectTimeout: "60"
  deployment:
    autoRestart: true
    replicas: 1
    labels: {}
    annotations: {}
    podSecurityContext: {}
    securityContext: {}
    resources:
      limits:
        cpu: 1000m
        memory: 2Gi
      requests:
        cpu: 500m
        memory: 1Gi
    command:
      - "/entrypoint.sh"
    startupProbe:
      httpGet:
        path: /health
        port: 8080
      failureThreshold: 10
      periodSeconds: 10
      timeoutSeconds: 1
    livenessProbe:
      httpGet:
        path: /health
        port: 8080
      failureThreshold: 10
      periodSeconds: 10
      timeoutSeconds: 1
    readinessProbe:
      httpGet:
        path: /health
        port: 8080
      failureThreshold: 10
      periodSeconds: 10
      timeoutSeconds: 1
    extraContainerConfig: {}
    extraEnv: []
    sidecars: []
    nodeSelector: {}
    tolerations: []
    topologySpreadConstraints: []
    affinity: {}
    volumes: []
    volumeMounts: []
    terminationGracePeriodSeconds: 30
  autoscaling:
    enabled: false
    # If enabled, use the following values to configure the HPA. You can also use your own HPA configuration by not creating an HPA.
    # You may want to manage the HPA yourself if you have a custom autoscaling setup like KEDA.
    createHpa: true
    minReplicas: 1
    maxReplicas: 5
    targetCPUUtilizationPercentage: 50
    targetMemoryUtilizationPercentage: 80
  service:
    type: LoadBalancer
    httpPort: 80
    httpsPort: 443
    labels: {}
    annotations: {}
    loadBalancerSourceRanges: []
    loadBalancerIP: ""
  serviceAccount:
    create: true
    name: ""
    labels: {}
    annotations: {}

platformBackend:
  name: "platform-backend"
  containerPort: 1986
  existingConfigMapName: ""
  deployment:
    autoRestart: true
    replicas: 3
    labels: {}
    annotations: {}
    podSecurityContext: {}
    securityContext: {}
    resources:
      limits:
        cpu: 2000m
        memory: 4Gi
      requests:
        cpu: 1000m
        memory: 2Gi
    command:
      - "./smith-go"
    startupProbe:
      httpGet:
        path: /ok
        port: 1986
      failureThreshold: 6
      periodSeconds: 10
      timeoutSeconds: 1
    livenessProbe:
      httpGet:
        path: /ok
        port: 1986
      failureThreshold: 6
      periodSeconds: 10
      timeoutSeconds: 1
    readinessProbe:
      httpGet:
        path: /ok
        port: 1986
      failureThreshold: 6
      periodSeconds: 10
      timeoutSeconds: 1
    extraContainerConfig: {}
    extraEnv: []
    sidecars: []
    nodeSelector: {}
    tolerations: []
    topologySpreadConstraints: []
    affinity: {}
    volumes: []
    volumeMounts: []
    terminationGracePeriodSeconds: 30
  autoscaling:
    enabled: false
    # If enabled, use the following values to configure the HPA. You can also use your own HPA configuration by not creating an HPA.
    # You may want to manage the HPA yourself if you have a custom autoscaling setup like KEDA.
    createHpa: true
    minReplicas: 3
    maxReplicas: 10
    targetCPUUtilizationPercentage: 50
    targetMemoryUtilizationPercentage: 80
  service:
    type: ClusterIP
    port: 1986
    labels: {}
    annotations: {}
    loadBalancerSourceRanges: []
    loadBalancerIP: ""
  serviceAccount:
    create: true
    name: ""
    labels: {}
    annotations: {}

playground:
  name: "playground"
  containerPort: 1988
  deployment:
    autoRestart: true
    replicas: 1
    labels: {}
    annotations: {}
    podSecurityContext: {}
    securityContext: {}
    resources:
      limits:
        cpu: 1000m
        memory: 8Gi
      requests:
        cpu: 500m
        memory: 1Gi
    command:
      - "uvicorn"
      - "playground.main:app"
      - "--host"
      - "0.0.0.0"
      - "--port"
      - "$(PORT)"
      - "--log-level"
      - "$(LOG_LEVEL)"
      - "--loop"
      - "uvloop"
      - "--http"
      - "httptools"
      - "--no-access-log"
    startupProbe:
      httpGet:
        path: /ok
        port: 1988
      failureThreshold: 6
      periodSeconds: 10
      timeoutSeconds: 1
    livenessProbe:
      httpGet:
        path: /ok
        port: 1988
      failureThreshold: 6
      periodSeconds: 10
      timeoutSeconds: 1
    readinessProbe:
      httpGet:
        path: /ok
        port: 1988
      failureThreshold: 6
      periodSeconds: 10
      timeoutSeconds: 1
    extraContainerConfig: {}
    extraEnv: []
    sidecars: []
    nodeSelector: {}
    tolerations: []
    topologySpreadConstraints: []
    affinity: {}
    volumes: []
    volumeMounts: []
    terminationGracePeriodSeconds: 30
  autoscaling:
    enabled: false
    # If enabled, use the following values to configure the HPA. You can also use your own HPA configuration by not creating an HPA.
    # You may want to manage the HPA yourself if you have a custom autoscaling setup like KEDA.
    createHpa: true
    minReplicas: 1
    maxReplicas: 5
    targetCPUUtilizationPercentage: 50
    targetMemoryUtilizationPercentage: 80
  service:
    type: ClusterIP
    port: 1988
    labels: {}
    annotations: {}
    loadBalancerSourceRanges: []
    loadBalancerIP: ""
  serviceAccount:
    create: true
    name: ""
    labels: {}
    annotations: {}

postgres:
  name: "postgres"
  external:
    # If enabled, use the following values to connect to an external database. This will also disable the
    # creation of a postgres stateful-set and service.
    enabled: false
    host: ""
    port: "5432"
    user: "postgres"
    password: "postgres"
    database: "postgres"
    schema: "public"
    # If connection string is specified, we will ignore all above values and use the connection string instead.
    # Do not include the driver name(something like "postgres://" in the connection string.
    connectionUrl: ""
    existingSecretName: ""
  containerPort: 5432
  statefulSet:
    labels: {}
    annotations: {}
    podSecurityContext: {}
    securityContext: {}
    resources:
      # In a production environment, we strongly recommend an external Postgres database.
      limits:
        cpu: 4000m
        memory: 16Gi
      requests:
        cpu: 2000m
        memory: 8Gi
    command: []
    startupProbe:
      exec:
        command:
          - /bin/sh
          - -c
          - exec pg_isready -d postgres -U postgres
      failureThreshold: 6
      periodSeconds: 10
      timeoutSeconds: 1
    readinessProbe:
      exec:
        command:
          - /bin/sh
          - -c
          - exec pg_isready -d postgres -U postgres
      failureThreshold: 6
      periodSeconds: 10
      timeoutSeconds: 1
    livenessProbe:
      exec:
        command:
          - /bin/sh
          - -c
          - exec pg_isready -d postgres -U postgres
      failureThreshold: 6
      periodSeconds: 10
      timeoutSeconds: 1
    extraContainerConfig: {}
    extraEnv: []
    sidecars: []
    nodeSelector: {}
    tolerations: []
    topologySpreadConstraints: []
    affinity: {}
    volumes: []
    volumeMounts: []
    persistence:
      size: 8Gi
      storageClassName: ""
  service:
    type: ClusterIP
    port: 5432
    labels: {}
    annotations: {}
    loadBalancerSourceRanges: []
    loadBalancerIP: ""
  serviceAccount:
    create: true
    name: ""
    labels: {}
    annotations: {}

queue:
  name: "queue"
  deployment:
    autoRestart: true
    replicas: 3
    labels: {}
    annotations: {}
    podSecurityContext: {}
    securityContext: {}
    resources:
      limits:
        cpu: 2000m
        memory: 4Gi
      requests:
        cpu: 1000m
        memory: 2Gi
    command:
      - "saq"
      - "app.workers.queues.single_queue_worker.settings"
      - "--quiet"
    startupProbe:
      exec:
        command:
          - "saq"
          - "app.workers.queues.single_queue_worker.settings"
          - "--check"
      failureThreshold: 6
      periodSeconds: 60
      timeoutSeconds: 30
    readinessProbe:
      exec:
        command:
          - "saq"
          - "app.workers.queues.single_queue_worker.settings"
          - "--check"
      failureThreshold: 6
      periodSeconds: 60
      timeoutSeconds: 30
    livenessProbe:
      exec:
        command:
          - "saq"
          - "app.workers.queues.single_queue_worker.settings"
          - "--check"
      failureThreshold: 6
      periodSeconds: 60
      timeoutSeconds: 30
    extraContainerConfig: {}
    extraEnv: []
    sidecars: []
    nodeSelector: {}
    tolerations: []
    topologySpreadConstraints: []
    affinity: {}
    volumes: []
    volumeMounts: []
    terminationGracePeriodSeconds: 30
  autoscaling:
    enabled: false
    # If enabled, use the following values to configure the HPA. You can also use your own HPA configuration by not creating an HPA.
    # You may want to manage the HPA yourself if you have a custom autoscaling setup like KEDA.
    createHpa: true
    minReplicas: 3
    maxReplicas: 10
    targetCPUUtilizationPercentage: 50
    targetMemoryUtilizationPercentage: 80
  serviceAccount:
    create: true
    name: ""
    labels: {}
    annotations: {}

redis:
  name: "redis"
  external:
    # If enabled, use the following values to connect to an external redis instance. This will also disable the
    # creation of a redis stateful-set and service.
    enabled: false
    connectionUrl: ""
    existingSecretName: ""
  containerPort: 6379
  statefulSet:
    labels: {}
    annotations: {}
    podSecurityContext: {}
    securityContext: {}
    # In a production environment, we strongly recommend an external Redis database.
    resources:
      limits:
        cpu: 4000m
        memory: 8Gi
      requests:
        cpu: 2000m
        memory: 4Gi
    command: []
    startupProbe:
      exec:
        command:
          - /bin/sh
          - -c
          - exec redis-cli ping
      failureThreshold: 6
      periodSeconds: 10
      timeoutSeconds: 1
    readinessProbe:
      exec:
        command:
          - /bin/sh
          - -c
          - exec redis-cli ping
      failureThreshold: 6
      periodSeconds: 10
      timeoutSeconds: 1
    livenessProbe:
      exec:
        command:
          - /bin/sh
          - -c
          - exec redis-cli ping
      failureThreshold: 6
      periodSeconds: 10
      timeoutSeconds: 1
    extraContainerConfig: {}
    extraEnv: []
    sidecars: []
    nodeSelector: {}
    tolerations: []
    topologySpreadConstraints: []
    affinity: {}
    volumes: []
    volumeMounts: []
    persistence:
      enabled: true
      size: 8Gi
      storageClassName: ""
  service:
    type: ClusterIP
    port: 6379
    labels: {}
    annotations: {}
    loadBalancerSourceRanges: []
    loadBalancerIP: ""
  serviceAccount:
    create: true
    name: ""
    labels: {}
    annotations: {}
