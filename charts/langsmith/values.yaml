# Default values for the langsmith helm chart. Refer to documentation on individual values for help with configuration.

# -- Provide a name in place of `langsmith`
nameOverride: ""
# -- String to fully override `"langsmith.fullname"`
fullnameOverride: ""
# -- Annotations that will be applied to all resources created by the chart
commonAnnotations: {}
# -- Labels that will be applied to all resources created by the chart
commonLabels: {}
# -- Common environment variables that will be applied to all deployments/statefulsets except for the playground/aceBackend services (which are sandboxed). Be careful not to override values already specified by the chart.
commonEnv: []

images:
   # -- Secrets with credentials to pull images from a private registry. Specified as name: value.
  imagePullSecrets: []
  aceBackendImage:
    repository: "docker.io/langchain/langsmith-ace-backend"
    pullPolicy: IfNotPresent
    tag: "0.9.78"
  backendImage:
    repository: "docker.io/langchain/langsmith-backend"
    pullPolicy: IfNotPresent
    tag: "0.9.78"
  frontendImage:
    repository: "docker.io/langchain/langsmith-frontend"
    pullPolicy: IfNotPresent
    tag: "0.9.78"
  hostBackendImage:
    repository: "docker.io/langchain/hosted-langserve-backend"
    pullPolicy: IfNotPresent
    tag: "0.9.78"
  operatorImage:
    repository: "docker.io/langchain/langgraph-operator"
    pullPolicy: IfNotPresent
    tag: "aa9dff4"
  platformBackendImage:
    repository: "docker.io/langchain/langsmith-go-backend"
    pullPolicy: IfNotPresent
    tag: "0.9.78"
  playgroundImage:
    repository: "docker.io/langchain/langsmith-playground"
    pullPolicy: IfNotPresent
    tag: "0.9.78"
  postgresImage:
    repository: "docker.io/postgres"
    pullPolicy: IfNotPresent
    tag: "14.7"
  redisImage:
    repository: "docker.io/redis"
    pullPolicy: IfNotPresent
    tag: "7"
  clickhouseImage:
    repository: "docker.io/clickhouse/clickhouse-server"
    pullPolicy: Always
    tag: "24.8"


ingress:
  enabled: false
  hostname: ""
  subdomain: ""
  ingressClassName: ""
  annotations: {}
  labels: {}
  tls: []

config:
  existingSecretName: ""
  langsmithLicenseKey: ""
  # -- Optional. Used to enable the Langgraph platform. If enabled, the license key must be provided.
  langgraphPlatform:
    enabled: false
    langgraphPlatformLicenseKey: ""

  # -- Salt used to generate the API key. Should be a random string.
  apiKeySalt: ""
  logLevel: "info"
  # -- Must be 'oauth' for OAuth with PKCE, 'mixed' for basic auth or OAuth without PKCE
  authType: ""
  basicAuth:
    enabled: false
    initialOrgAdminEmail: ""
    # Either set the password and JWT secret in plaintext, or set in the secret specified by existingSecretName above.
    initialOrgAdminPassword: ""
    jwtSecret: ""
  # -- Prevent organization creation. If using basic auth, this is set to true by default.
  orgCreationDisabled: false
  # -- Disable personal orgs. Users will need to be invited to an org manually. If using basic auth, this is set to true by default.
  personalOrgsDisabled: false
  # -- Enable Workspace Admins to invite users to the org and workspace.
  workspaceScopeOrgInvitesEnabled: false

  # -- Base URL of the LangSmith installation. Used for redirects.
  hostname: ""

  oauth:
    enabled: false
    oauthClientId: ""
    # -- Client secret requires authType to be 'mixed' and hostname to be present
    oauthClientSecret: ""
    oauthIssuerUrl: ""
    oauthScopes: "email,profile,openid"  # Comma-separated list of scopes. We require at least 'email', 'profile', and 'openid'. Some OIDC providers require the 'offline_access' scope for refresh tokens.
    oauthSessionMaxSec: "86400"  # 24 hours. Maximum age of a session in seconds. Your session will attempt to refresh until the max age at which point you will be logged out.
  # -- TTL configuration
  # Optional. Used to set TTLS for longlived and shortlived objects.
  ttl:
    enabled: true
    ttl_period_seconds:
      # -- 400 day longlived and 14 day shortlived
      longlived: "34560000"
      shortlived: "1209600"
    #
  # -- Blob storage configuration
  # Optional. Used to store inputs, outputs, and errors in Blob Storage.
  # We currently support S3, GCS, Minio, and Azure as Blob Storage providers.
  blobStorage:
    enabled: false
    engine: "S3"
    # If you are using langsmith-managed-clickhouse, you may not want inputs to be stored in clickhouse for search.
    # Set this as false to ensure that inputs/outputs/errors are not stored in clickhouse.
    # 'clickhouse.external.hybrid: true' overrides this to false.
    chSearchEnabled: true
    # Set this to change the threshold for payloads to be stored in blob storage
    # 'clickhouse.external.hybrid: true' overrides this to 0
    minBlobStorageSizeKb: "20"
    # If you are using workload identity, you may not need to store the S3 credentials in the secrets.
    # Instead, you will need to add the workload identity annotation to the backend and queue service accounts.
    accessKey: ""
    accessKeySecret: ""
    bucketName: ""
    apiURL: "https://s3.us-west-2.amazonaws.com"
    # The following blob storage configuration values are for Azure and require blobStorage.engine = "Azure"
    # -- Optional. Set this along with azureStorageAccountKey to use a storage account and access key. Higher precedence than azureStorageConnectionString.
    azureStorageAccountName: ""
    azureStorageAccountKey: ""
    # -- Required if using Azure blob storage
    azureStorageContainerName: ""
    # -- Optional. Use this to specify the full connection string including any authentication params.
    azureStorageConnectionString: ""
    # -- Optional. Use this to customize the service URL, which by default is 'https://<storage_account_name>.blob.core.windows.net/'
    azureStorageServiceUrlOverride: ""

  # -- Application Settings. These are used to tune the application
  settings:
    # -- Optional. Be very careful when lowering this value as it can result in runs being lost if your queue is down/not processing items fast enough.
    redisRunsExpirySeconds: "43200"  # 12 hours

aceBackend:
  name: "ace-backend"
  containerPort: 1987
  bindAddress: "0.0.0.0"
  deployment:
    autoRestart: true
    replicas: 1
    labels: {}
    annotations: {}
    podSecurityContext: {}
    securityContext: {}
    resources:
      limits:
        cpu: 1000m
        memory: 2Gi
      requests:
        cpu: 200m
        memory: 1000Mi
    command:
      - "deno"
      - "run"
      - "--unstable-worker-options"
      - "--allow-env"
      - "--allow-net=$(BIND_ADDRESS):$(PORT)"
      - "--node-modules-dir"
      - "-R"
      - "src/main.ts"
      - "-R"
      - "src/python_worker.ts"
    startupProbe:
      httpGet:
        path: /ok
        port: 1987
      failureThreshold: 6
      periodSeconds: 10
      timeoutSeconds: 1
    livenessProbe:
      httpGet:
        path: /ok
        port: 1987
      failureThreshold: 6
      periodSeconds: 10
      timeoutSeconds: 1
    readinessProbe:
      httpGet:
        path: /ok
        port: 1987
      failureThreshold: 6
      periodSeconds: 10
      timeoutSeconds: 1
    extraContainerConfig: {}
    extraEnv: []
    sidecars: []
    nodeSelector: {}
    tolerations: []
    topologySpreadConstraints: []
    affinity: {}
    volumes: []
    volumeMounts: []
    terminationGracePeriodSeconds: 30
  autoscaling:
    enabled: false
    # If enabled, use the following values to configure the HPA. You can also use your own HPA configuration by not creating an HPA.
    # You may want to manage the HPA yourself if you have a custom autoscaling setup like KEDA.
    createHpa: true
    minReplicas: 1
    maxReplicas: 5
    targetCPUUtilizationPercentage: 50
    targetMemoryUtilizationPercentage: 80
  pdb:
    enabled: false
    minAvailable: 1
    labels: {}
    annotations: {}
  service:
    type: ClusterIP
    port: 1987
    labels: {}
    annotations: {}
    loadBalancerSourceRanges: []
    loadBalancerIP: ""
  serviceAccount:
    create: true
    name: ""
    labels: {}
    annotations: {}


backend:
  name: "backend"
  containerPort: 1984
  existingConfigMapName: ""
  deployment:
    autoRestart: true
    replicas: 1
    labels: {}
    annotations: {}
    podSecurityContext: {}
    securityContext: {}
    resources:
      limits:
        cpu: 2000m
        memory: 4Gi
      requests:
        cpu: 1000m
        memory: 2Gi
    command:
      - "uvicorn"
      - "app.main:app"
      - "--host"
      - "0.0.0.0"
      - "--port"
      - "$(PORT)"
      - "--log-level"
      - "$(LOG_LEVEL)"
      - "--loop"
      - "uvloop"
      - "--http"
      - "httptools"
      - "--no-access-log"
    startupProbe:
      httpGet:
        path: /health
        port: 1984
      failureThreshold: 6
      periodSeconds: 10
      timeoutSeconds: 1
    livenessProbe:
      httpGet:
        path: /health
        port: 1984
      failureThreshold: 6
      periodSeconds: 10
      timeoutSeconds: 1
    readinessProbe:
      httpGet:
        path: /health
        port: 1984
      failureThreshold: 6
      periodSeconds: 10
      timeoutSeconds: 1
    extraContainerConfig: {}
    extraEnv: []
    sidecars: []
    nodeSelector: {}
    tolerations: []
    topologySpreadConstraints: []
    affinity: {}
    volumes: []
    volumeMounts: []
    terminationGracePeriodSeconds: 30
  migrations:
    enabled: true
    # Helpful when running using helm template to avoid the job name conflict
    randomizeName: true
    labels: {}
    annotations: {}
    podSecurityContext: {}
    securityContext: {}
    resources:
      limits:
        cpu: 1000m
        memory: 1Gi
      requests:
        cpu: 200m
        memory: 500Mi
    command:
      - "/bin/bash"
      - "-c"
      - "alembic upgrade head"
    extraContainerConfig: {}
    extraEnv: []
    sidecars: []
    nodeSelector: {}
    tolerations: []
    topologySpreadConstraints: []
    affinity: {}
    volumes: []
    volumeMounts: []
    # You can set this to null to disable the TTL for the migrations job. Useful for some deployment setups.
    ttlSecondsAfterFinished: 600
  authBootstrap:
    # Helpful when running using helm template to avoid the job name conflict
    randomizeName: true
    labels: {}
    annotations: {}
    podSecurityContext: {}
    securityContext: {}
    resources:
      limits:
        cpu: 1000m
        memory: 1Gi
      requests:
        cpu: 200m
        memory: 500Mi
    command:
      - "python"
      - "hooks/auth_bootstrap.pyc"
    extraContainerConfig: {}
    extraEnv: []
    sidecars: []
    nodeSelector: {}
    tolerations: []
    topologySpreadConstraints: []
    affinity: {}
    volumes: []
    volumeMounts: []
    # You can set this to null to disable the TTL for the migrations job. Useful for some deployment setups.
    ttlSecondsAfterFinished: 600
  clickhouseMigrations:
    enabled: true
    # Helpful when running using helm template to avoid the job name conflict
    randomizeName: true
    labels: {}
    annotations: {}
    podSecurityContext: {}
    securityContext: {}
    resources:
      limits:
        cpu: 1000m
        memory: 1Gi
      requests:
        cpu: 200m
        memory: 500Mi
    command:
      - "/bin/bash"
      - "scripts/wait_for_clickhouse_and_migrate.sh"
    extraContainerConfig: {}
    extraEnv: []
    sidecars: []
    nodeSelector: {}
    tolerations: []
    topologySpreadConstraints: []
    affinity: {}
    volumes: []
    volumeMounts: []
    # You can set this to null to disable the TTL for the migrations job. Useful for some deployment setups.
    ttlSecondsAfterFinished: 600
  autoscaling:
    enabled: false
    # If enabled, use the following values to configure the HPA. You can also use your own HPA configuration by not creating an HPA.
    # You may want to manage the HPA yourself if you have a custom autoscaling setup like KEDA.
    createHpa: true
    minReplicas: 1
    maxReplicas: 5
    targetCPUUtilizationPercentage: 50
    targetMemoryUtilizationPercentage: 80
  pdb:
    enabled: false
    minAvailable: 1
    labels: {}
    annotations: {}
  service:
    type: ClusterIP
    port: 1984
    labels: {}
    annotations: {}
    loadBalancerSourceRanges: []
    loadBalancerIP: ""
  serviceAccount:
    create: true
    name: ""
    labels: {}
    annotations: {}

clickhouse:
  name: "clickhouse"
  config:
    allowSimdjson: true
  external:
    # If enabled, use the following values to connect to an external database. This will also disable the
    # creation of a clickhouse stateful-set and service.
    enabled: false
    # -- Must be set to true if using managed ClickHouse
    hybrid: false
    host: ""
    hostSecretKey: "clickhouse_host"
    port: "8123"
    portSecretKey: "clickhouse_port"
    nativePort: "9000"
    nativePortSecretKey: "clickhouse_native_port"
    user: "default"
    userSecretKey: "clickhouse_user"
    password: "password"
    passwordSecretKey: "clickhouse_password"
    database: "default"
    databaseSecretKey: "clickhouse_db"
    tls: false
    tlsSecretKey: "clickhouse_tls"
    cluster: ""
    existingSecretName: ""
  containerHttpPort: 8123
  containerNativePort: 9000
  statefulSet:
    labels: {}
    annotations: {}
    podSecurityContext: {}
    securityContext: {}
    resources:
# For a production environment, we recommend increasing the CPU and memory limits to something like 16 cores and 64Gi.
#      limits:
#        cpu: 16000m
#        memory: 64Gi
#      requests:
#        cpu: 8000m
#        memory: 32Gi
      limits:
        cpu: 8000m
        memory: 32Gi
      requests:
        cpu: 3500m
        memory: 12Gi
    command:
      - "/bin/bash"
      - "-c"
      - "sed 's/id -g/id -gn/' /entrypoint.sh > /tmp/entrypoint.sh; exec bash /tmp/entrypoint.sh"
    startupProbe:
      httpGet:
        path: /ping
        port: 8123
      failureThreshold: 6
      periodSeconds: 10
      timeoutSeconds: 1
    readinessProbe:
      httpGet:
        path: /ping
        port: 8123
      failureThreshold: 6
      periodSeconds: 10
      timeoutSeconds: 1
    livenessProbe:
      httpGet:
        path: /ping
        port: 8123
      failureThreshold: 6
      periodSeconds: 10
      timeoutSeconds: 1
    extraContainerConfig: {}
    extraEnv: []
    sidecars: []
    nodeSelector: {}
    tolerations: []
    topologySpreadConstraints: []
    affinity: {}
    volumes: []
    volumeMounts: []
    # We recommend using a persistent volume and increasing the storage size to something like 50Gi when using in a production environment!
    persistence:
      enabled: true
      size: 50Gi
      storageClassName: ""
  pdb:
    enabled: false
    minAvailable: 1
    labels: {}
    annotations: {}
  service:
    type: ClusterIP
    httpPort: 8123
    nativePort: 9000
    labels: {}
    annotations: {}
    loadBalancerSourceRanges: []
    loadBalancerIP: ""
  serviceAccount:
    create: true
    name: ""
    labels: {}
    annotations: {}

frontend:
  name: "frontend"
  containerPort: 8080
  existingConfigMapName: ""
  # Nginx Max Body Size. Refer to https://nginx.org/en/docs/http/ngx_http_core_module.html#client_max_body_size for more information.
  maxBodySize: "25M"
  proxyReadTimeout: "300"
  proxyWriteTimeout: "300"
  proxyConnectTimeout: "60"
  ipv6Enabled: true
  deployment:
    autoRestart: true
    replicas: 1
    labels: {}
    annotations: {}
    podSecurityContext: {}
    securityContext: {}
    resources:
      limits:
        cpu: 1000m
        memory: 2Gi
      requests:
        cpu: 500m
        memory: 1Gi
    command:
      - "/entrypoint.sh"
    startupProbe:
      httpGet:
        path: /health
        port: 8080
      failureThreshold: 10
      periodSeconds: 10
      timeoutSeconds: 1
    livenessProbe:
      httpGet:
        path: /health
        port: 8080
      failureThreshold: 10
      periodSeconds: 10
      timeoutSeconds: 1
    readinessProbe:
      httpGet:
        path: /health
        port: 8080
      failureThreshold: 10
      periodSeconds: 10
      timeoutSeconds: 1
    extraContainerConfig: {}
    extraEnv: []
    sidecars: []
    nodeSelector: {}
    tolerations: []
    topologySpreadConstraints: []
    affinity: {}
    volumes: []
    volumeMounts: []
    terminationGracePeriodSeconds: 30
  autoscaling:
    enabled: false
    # If enabled, use the following values to configure the HPA. You can also use your own HPA configuration by not creating an HPA.
    # You may want to manage the HPA yourself if you have a custom autoscaling setup like KEDA.
    createHpa: true
    minReplicas: 1
    maxReplicas: 5
    targetCPUUtilizationPercentage: 50
    targetMemoryUtilizationPercentage: 80
  pdb:
    enabled: false
    minAvailable: 1
    labels: {}
    annotations: {}
  service:
    type: LoadBalancer
    httpPort: 80
    httpsPort: 443
    labels: {}
    annotations: {}
    loadBalancerSourceRanges: []
    loadBalancerIP: ""
  serviceAccount:
    create: true
    name: ""
    labels: {}
    annotations: {}

hostBackend:
  name: "host-backend"
  containerPort: 1985
  deployment:
    autoRestart: true
    replicas: 1
    labels: {}
    annotations: {}
    podSecurityContext: {}
    securityContext: {}
    resources:
      limits:
        cpu: 1000m
        memory: 2Gi
      requests:
        cpu: 200m
        memory: 1000Mi
    command:
      - "uvicorn"
      - "host.main:app"
      - "--host"
      - "0.0.0.0"
      - "--port"
      - "$(PORT)"
      - "--log-level"
      - "$(LOG_LEVEL)"
      - "--loop"
      - "uvloop"
      - "--http"
      - "httptools"
      - "--no-access-log"
    startupProbe:
      httpGet:
        path: /ok
        port: 1985
      failureThreshold: 6
      periodSeconds: 10
      timeoutSeconds: 1
    livenessProbe:
      httpGet:
        path: /ok
        port: 1985
      failureThreshold: 6
      periodSeconds: 10
      timeoutSeconds: 1
    readinessProbe:
      httpGet:
        path: /ok
        port: 1985
      failureThreshold: 6
      periodSeconds: 10
      timeoutSeconds: 1
    extraContainerConfig: {}
    extraEnv: []
    sidecars: []
    nodeSelector: {}
    tolerations: []
    topologySpreadConstraints: []
    affinity: {}
    volumes: []
    volumeMounts: []
    terminationGracePeriodSeconds: 30
  autoscaling:
    enabled: false
    # If enabled, use the following values to configure the HPA. You can also use your own HPA configuration by not creating an HPA.
    # You may want to manage the HPA yourself if you have a custom autoscaling setup like KEDA.
    createHpa: true
    minReplicas: 1
    maxReplicas: 5
    targetCPUUtilizationPercentage: 50
    targetMemoryUtilizationPercentage: 80
  pdb:
    enabled: false
    minAvailable: 1
    labels: {}
    annotations: {}
  service:
    type: ClusterIP
    port: 1985
    labels: {}
    annotations: {}
    loadBalancerSourceRanges: []
    loadBalancerIP: ""
  serviceAccount:
    create: true
    name: ""
    labels: {}
    annotations: {}
  rbac:
    create: true
    labels: {}
    annotations: {}

listener:
  name: "listener"
  deployment:
    autoRestart: true
    replicas: 1
    labels: {}
    annotations: {}
    podSecurityContext: {}
    securityContext: {}
    resources:
      limits:
        cpu: 2000m
        memory: 4Gi
      requests:
        cpu: 1000m
        memory: 2Gi
    command:
      - "saq"
      - "app.workers.queues.host_worker.settings"
      - "--quiet"
    startupProbe:
      exec:
        command:
          - "saq"
          - "app.workers.queues.host_worker.settings"
          - "--check"
      failureThreshold: 6
      periodSeconds: 60
      timeoutSeconds: 30
    readinessProbe:
      exec:
        command:
          - "saq"
          - "app.workers.queues.host_worker.settings"
          - "--check"
      failureThreshold: 6
      periodSeconds: 60
      timeoutSeconds: 30
    livenessProbe:
      exec:
        command:
          - "saq"
          - "app.workers.queues.host_worker.settings"
          - "--check"
      failureThreshold: 6
      periodSeconds: 60
      timeoutSeconds: 30
    extraContainerConfig: {}
    extraEnv: []
    sidecars: []
    nodeSelector: {}
    tolerations: []
    topologySpreadConstraints: []
    affinity: {}
    volumes: []
    volumeMounts: []
    terminationGracePeriodSeconds: 30
  autoscaling:
    enabled: false
    # If enabled, use the following values to configure the HPA. You can also use your own HPA configuration by not creating an HPA.
    # You may want to manage the HPA yourself if you have a custom autoscaling setup like KEDA.
    createHpa: true
    minReplicas: 3
    maxReplicas: 10
    targetCPUUtilizationPercentage: 50
    targetMemoryUtilizationPercentage: 80
  pdb:
    enabled: false
    minAvailable: 1
    labels: {}
    annotations: {}
  serviceAccount:
    create: true
    name: ""
    labels: {}
    annotations: {}
  rbac:
    create: true
    labels: {}
    annotations: {}

platformBackend:
  name: "platform-backend"
  containerPort: 1986
  existingConfigMapName: ""
  deployment:
    autoRestart: true
    replicas: 3
    labels: {}
    annotations: {}
    podSecurityContext: {}
    securityContext: {}
    resources:
      limits:
        cpu: 2000m
        memory: 4Gi
      requests:
        cpu: 1000m
        memory: 2Gi
    command:
      - "./smith-go"
    startupProbe:
      httpGet:
        path: /ok
        port: 1986
      failureThreshold: 6
      periodSeconds: 10
      timeoutSeconds: 1
    livenessProbe:
      httpGet:
        path: /ok
        port: 1986
      failureThreshold: 6
      periodSeconds: 10
      timeoutSeconds: 1
    readinessProbe:
      httpGet:
        path: /ok
        port: 1986
      failureThreshold: 6
      periodSeconds: 10
      timeoutSeconds: 1
    extraContainerConfig: {}
    extraEnv: []
    sidecars: []
    nodeSelector: {}
    tolerations: []
    topologySpreadConstraints: []
    affinity: {}
    volumes: []
    volumeMounts: []
    terminationGracePeriodSeconds: 30
  autoscaling:
    enabled: false
    # If enabled, use the following values to configure the HPA. You can also use your own HPA configuration by not creating an HPA.
    # You may want to manage the HPA yourself if you have a custom autoscaling setup like KEDA.
    createHpa: true
    minReplicas: 3
    maxReplicas: 10
    targetCPUUtilizationPercentage: 50
    targetMemoryUtilizationPercentage: 80
  pdb:
    enabled: false
    minAvailable: 1
    labels: {}
    annotations: {}
  service:
    type: ClusterIP
    port: 1986
    labels: {}
    annotations: {}
    loadBalancerSourceRanges: []
    loadBalancerIP: ""
  serviceAccount:
    create: true
    name: ""
    labels: {}
    annotations: {}

operator:
  name: "operator"
  enabled: true
  createCRDs: true
  watchNamespaces: ""
  deployment:
    autoRestart: true
    replicas: 1
    labels: {}
    annotations: {}
    podSecurityContext: {}
    securityContext: {}
    resources:
      limits:
        cpu: 2000m
        memory: 4Gi
      requests:
        cpu: 1000m
        memory: 2Gi
    extraContainerConfig: {}
    extraEnv: []
    sidecars: []
    nodeSelector: {}
    tolerations: []
    topologySpreadConstraints: []
    affinity: {}
    volumes: []
    volumeMounts: []
    terminationGracePeriodSeconds: 30
  pdb:
    enabled: false
    minAvailable: 1
  serviceAccount:
    create: true
    name: ""
    labels: {}
    annotations: {}
  rbac:
    create: true
    labels: {}
    annotations: {}
  # These templates are used by the operator as the base spec for creating resources.
  templates:
    deployment: |
      apiVersion: apps/v1
      kind: Deployment
      metadata:
        name: ${name}
        namespace: ${namespace}
      spec:
        replicas: ${replicas}
        selector:
          matchLabels:
            app: ${name}
        template:
          metadata:
            labels:
              app: ${name}
          spec:
            enableServiceLinks: false
            containers:
            - name: api-server
              image: ${image}
              ports:
              - name: api-server
                containerPort: 8000
                protocol: TCP
              livenessProbe:
                httpGet:
                  path: /ok?check_db=1
                  port: 8000
                initialDelaySeconds: 90
                periodSeconds: 5
                timeoutSeconds: 5
              readinessProbe:
                httpGet:
                  path: /ok
                  port: 8000
                initialDelaySeconds: 90
                periodSeconds: 5
                timeoutSeconds: 5
    service: |
      apiVersion: v1
      kind: Service
      metadata:
        name: ${name}
        namespace: ${namespace}
      spec:
        type: ClusterIP
        selector:
          app: ${name}
        ports:
        - name: api-server
          protocol: TCP
          port: 8000
          targetPort: 8000
    ingress: |
      apiVersion: networking.k8s.io/v1
      kind: Ingress
      metadata:
        name: ${name}
        namespace: ${namespace}
      spec:
        ingressClassName: ${ingress.ingressClassName}
        rules:
        - http:
            paths:
            - path: /
              pathType: Prefix
              backend:
                service:
                  name: ${name}
                  port:
                    number: 8000

playground:
  name: "playground"
  containerPort: 1988
  deployment:
    autoRestart: true
    replicas: 1
    labels: {}
    annotations: {}
    podSecurityContext: {}
    securityContext: {}
    resources:
      limits:
        cpu: 1000m
        memory: 8Gi
      requests:
        cpu: 500m
        memory: 1Gi
    command:
      - "uvicorn"
      - "playground.main:app"
      - "--host"
      - "0.0.0.0"
      - "--port"
      - "$(PORT)"
      - "--log-level"
      - "$(LOG_LEVEL)"
      - "--loop"
      - "uvloop"
      - "--http"
      - "httptools"
      - "--no-access-log"
    startupProbe:
      httpGet:
        path: /ok
        port: 1988
      failureThreshold: 6
      periodSeconds: 10
      timeoutSeconds: 1
    livenessProbe:
      httpGet:
        path: /ok
        port: 1988
      failureThreshold: 6
      periodSeconds: 10
      timeoutSeconds: 1
    readinessProbe:
      httpGet:
        path: /ok
        port: 1988
      failureThreshold: 6
      periodSeconds: 10
      timeoutSeconds: 1
    extraContainerConfig: {}
    extraEnv: []
    sidecars: []
    nodeSelector: {}
    tolerations: []
    topologySpreadConstraints: []
    affinity: {}
    volumes: []
    volumeMounts: []
    terminationGracePeriodSeconds: 30
  autoscaling:
    enabled: false
    # If enabled, use the following values to configure the HPA. You can also use your own HPA configuration by not creating an HPA.
    # You may want to manage the HPA yourself if you have a custom autoscaling setup like KEDA.
    createHpa: true
    minReplicas: 1
    maxReplicas: 5
    targetCPUUtilizationPercentage: 50
    targetMemoryUtilizationPercentage: 80
  pdb:
    enabled: false
    minAvailable: 1
    labels: {}
    annotations: {}
  service:
    type: ClusterIP
    port: 1988
    labels: {}
    annotations: {}
    loadBalancerSourceRanges: []
    loadBalancerIP: ""
  serviceAccount:
    create: true
    name: ""
    labels: {}
    annotations: {}

postgres:
  name: "postgres"
  external:
    # If enabled, use the following values to connect to an external database. This will also disable the
    # creation of a postgres stateful-set and service.
    enabled: false
    host: ""
    port: "5432"
    user: "postgres"
    password: "postgres"
    database: "postgres"
    schema: "public"
    # If connection string is specified, we will ignore all above values and use the connection string instead.
    # Do not include the driver name(something like "postgres://" in the connection string.
    connectionUrl: ""
    connectionUrlSecretKey: "connection_url"
    existingSecretName: ""
  containerPort: 5432
  statefulSet:
    labels: {}
    annotations: {}
    podSecurityContext: {}
    securityContext: {}
    resources:
      # In a production environment, we strongly recommend an external Postgres database.
      limits:
        cpu: 4000m
        memory: 16Gi
      requests:
        cpu: 2000m
        memory: 8Gi
    command: []
    startupProbe:
      exec:
        command:
          - /bin/sh
          - -c
          - exec pg_isready -d postgres -U postgres
      failureThreshold: 6
      periodSeconds: 10
      timeoutSeconds: 1
    readinessProbe:
      exec:
        command:
          - /bin/sh
          - -c
          - exec pg_isready -d postgres -U postgres
      failureThreshold: 6
      periodSeconds: 10
      timeoutSeconds: 1
    livenessProbe:
      exec:
        command:
          - /bin/sh
          - -c
          - exec pg_isready -d postgres -U postgres
      failureThreshold: 6
      periodSeconds: 10
      timeoutSeconds: 1
    extraContainerConfig: {}
    extraEnv: []
    sidecars: []
    nodeSelector: {}
    tolerations: []
    topologySpreadConstraints: []
    affinity: {}
    volumes: []
    volumeMounts: []
    persistence:
      enabled: true
      size: 8Gi
      storageClassName: ""
  pdb:
    enabled: false
    minAvailable: 1
    labels: {}
    annotations: {}
  service:
    type: ClusterIP
    port: 5432
    labels: {}
    annotations: {}
    loadBalancerSourceRanges: []
    loadBalancerIP: ""
  serviceAccount:
    create: true
    name: ""
    labels: {}
    annotations: {}

queue:
  name: "queue"
  deployment:
    autoRestart: true
    replicas: 3
    labels: {}
    annotations: {}
    podSecurityContext: {}
    securityContext: {}
    resources:
      limits:
        cpu: 2000m
        memory: 4Gi
      requests:
        cpu: 1000m
        memory: 2Gi
    command:
      - "saq"
      - "app.workers.queues.single_queue_worker.settings"
      - "--quiet"
    startupProbe:
      exec:
        command:
          - "saq"
          - "app.workers.queues.single_queue_worker.settings"
          - "--check"
      failureThreshold: 6
      periodSeconds: 60
      timeoutSeconds: 30
    readinessProbe:
      exec:
        command:
          - "saq"
          - "app.workers.queues.single_queue_worker.settings"
          - "--check"
      failureThreshold: 6
      periodSeconds: 60
      timeoutSeconds: 30
    livenessProbe:
      exec:
        command:
          - "saq"
          - "app.workers.queues.single_queue_worker.settings"
          - "--check"
      failureThreshold: 6
      periodSeconds: 60
      timeoutSeconds: 30
    extraContainerConfig: {}
    extraEnv: []
    sidecars: []
    nodeSelector: {}
    tolerations: []
    topologySpreadConstraints: []
    affinity: {}
    volumes: []
    volumeMounts: []
    terminationGracePeriodSeconds: 30
  pdb:
    enabled: false
    minAvailable: 1
    labels: {}
    annotations: {}
  autoscaling:
    enabled: false
    # If enabled, use the following values to configure the HPA. You can also use your own HPA configuration by not creating an HPA.
    # You may want to manage the HPA yourself if you have a custom autoscaling setup like KEDA.
    createHpa: true
    minReplicas: 3
    maxReplicas: 10
    targetCPUUtilizationPercentage: 50
    targetMemoryUtilizationPercentage: 80
  serviceAccount:
    create: true
    name: ""
    labels: {}
    annotations: {}

redis:
  name: "redis"
  external:
    # If enabled, use the following values to connect to an external redis instance. This will also disable the
    # creation of a redis stateful-set and service.
    enabled: false
    connectionUrl: ""
    connectionUrlSecretKey: "connection_url"
    existingSecretName: ""
  containerPort: 6379
  statefulSet:
    labels: {}
    annotations: {}
    podSecurityContext: {}
    securityContext: {}
    # In a production environment, we strongly recommend an external Redis database.
    resources:
      limits:
        cpu: 4000m
        memory: 8Gi
      requests:
        cpu: 2000m
        memory: 4Gi
    command: []
    startupProbe:
      exec:
        command:
          - /bin/sh
          - -c
          - exec redis-cli ping
      failureThreshold: 6
      periodSeconds: 10
      timeoutSeconds: 1
    readinessProbe:
      exec:
        command:
          - /bin/sh
          - -c
          - exec redis-cli ping
      failureThreshold: 6
      periodSeconds: 10
      timeoutSeconds: 1
    livenessProbe:
      exec:
        command:
          - /bin/sh
          - -c
          - exec redis-cli ping
      failureThreshold: 6
      periodSeconds: 10
      timeoutSeconds: 1
    extraContainerConfig: {}
    extraEnv: []
    sidecars: []
    nodeSelector: {}
    tolerations: []
    topologySpreadConstraints: []
    affinity: {}
    volumes: []
    volumeMounts: []
    persistence:
      enabled: true
      size: 8Gi
      storageClassName: ""
  pdb:
    enabled: false
    minAvailable: 1
    labels: {}
    annotations: {}
  service:
    type: ClusterIP
    port: 6379
    labels: {}
    annotations: {}
    loadBalancerSourceRanges: []
    loadBalancerIP: ""
  serviceAccount:
    create: true
    name: ""
    labels: {}
    annotations: {}
